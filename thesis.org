* Introduction
** START

   The START Natural Language System is a software system designed
   to answer questions that are posed to it in natural
   language. START parses incoming questions, matches the queries
   created from the parse trees against its knowledge base and
   presents the appropriate information segments to the user. In
   this way, START provides untrained users with speedy access to
   knowledge that in many cases would take an expert some time to
   find.

   START (SynTactic Analysis using Reversible Transformations) was
   developed by Boris Katz at MIT's Artificial Intelligence
   Laboratory. Currently, the system is undergoing further development by
   the InfoLab Group, led by Boris Katz. START was first connected to the
   World Wide Web in December, 1993, and in its several forms has to date
   answered millions of questions from users around the world.

   A key technique called "natural language annotation" helps START
   connect information seekers to information sources. This technique
   employs natural language sentences and phrases annotations as
   descriptions of content that are associated with information
   segments at various granularities. An information segment is
   retrieved when its annotation matches an input question. This
   method allows START to handle all variety of media, including
   text, diagrams, images, video and audio clips, data sets, Web
   pages, and others.

   The natural language processing component of START consists of two
   modules that share the same grammar. The understanding module analyzes
   English text and produces a knowledge base that encodes information
   found in the text. Given an appropriate segment of the knowledge base,
   the generating module produces English sentences. Used in conjunction
   with the technique of natural language annotation, these modules put
   the power of sentence-level natural language processing to use in the
   service of multi-media information access.

* Wikipediabase

  WikipediaBase base is a backend to START responsible for providing
  access to wikipedia related information. The WikipediaBase we refer
  to is a python rewrite of the now deprecated Ruby WikipediaBase.


** People

   # TODO: rephrase this
   The python implementation was initially written by Chris
   Perivolaropoulos and was eventually handed over to

   - Alvaro Morales
   - Michael Silver
   # TODO: add the rest of the people


** Functionality

   The WikipediaBase server provides operations to:

   - find all classes that a Wikipedia term (object) belongs to, and
   - given a specific wikipedia term and a class under which it
     functions, provide information

   # XXX: Examples

*** Queries

*** Ontology

**** Terms

**** Classes

*** Infoboxes

    Infoboxes are tables displaying information about a wikipedia page
    in a semi structured way. At the moment most of the data retrieved
    by WikipadiaBase comes from infoboxes

    {{TODO: example image}}

** Getting started

*** Language

    The WikipediaBase implementation that we refer to is written in
    python. Previous implementations were written in Java and Ruby but
    the language of choice for the rewrite was python for multiple
    reasons:


    - Python is in the pre-graduate curriculum of MIT computer science
      department. This will ease the learning curve of new members of
      Infolab.
    - Python is a easy to learn and mature language with a rich and
      evolving ecosystem. This fact eases the introduction of new
      maintainers even further.

*** Getting the code

   The entire WikipediaBase resides in a git repository in infolab's
   github orginization page

   #+BEGIN_SRC sh
   git clone git@github.com:infolab-csail/WikipediaBase
   #+END_SRC

*** Virtualenv and dependencies

   WikipediaBase depends on multiple other python
   packages. Fortunately, python is shipped not only with a great
   package manager, but also with a mechanism called virtualenv that
   isolates installations of a project's dependencies from the rest of
   the system, thus avoiding problems like version or namespace
   collisions. The way this effectively works is that the global
   python installation is half copied half symlinked to a local
   directory and the dependencies are installed only in the local
   sandbox. To create and activate a python virtualenv:

   #+BEGIN_SRC sh
   $ virtualenv --no-site-packages py
   $ . py/bin/activate
   $ which python
   /the/local/directory/py/bin/python
   #+END_SRC

   Now that we can safely install anything we want without breaking
   any global installation

   #+BEGIN_SRC sh
   pip install -r requirements.txt
   #+END_SRC

   We will need some extra stuff for WikipediaBase to work:

   - Postresql
   - Redis

   The installation of these packages varies across platforms. Both
   these packages are databases. Their purpose is for caching and for
   storing ahead-of-time compitations like infobox markup name to
   rendered name matching.

*** Backend databases or live data

** Architecture


*** Pipeline

    When resolving a query WikipediaBase employs a pipeline of modules
    to figure out what the best way to respond would be.

**** Frontend

     # Find the port

     WikipediaBase can be used as a library but it's primary function
     is as a backend to START. The communication between START and
     WikipediaBase is carried out over a plaintext telnet connection on
     port {port} using EDN-like sexpressions. The frontend handles the
     network connection with START, translates the received queries
     into calls to knowledgebase and then translate the knowledgebase
     response into properly formulated sexpressions that it sends back
     over the telnet connection.

***** Protocol

**** Knowledgebase

     The knowledgebase is the entry point to the rest of
     wikipediabase. It uses the Provider/Acquirer pattern to
     transaprently provide the frontend with arbitrary methods. Those
     methods are responsible for chosing whether we are to resort to
     classifiers or resolvers (or any other mechanism) for answering
     the query. Available classifiers and resolvers become accessible
     to the knowledgebase automatically using their base class.

**** Classifiers

     Each classifier is a singleton that implements a heuristic for
     assigning a class of an object. Thereare a couple classifiers
     available at the moment.

**** Resolvers

     Resolvers are also singletons but their purpose is to find the
     value of the requested property.

**** Lisp types

     Lisp type instances are wrappers for python objects or values
     that are presentable in s-expression form that START can
     understand. They are created either from the raw received query
     and unwrapped to be useful to the pipeline, or by the answer
     WikipediaBase comes up with and then encoded into a string sent
     over telnet to START.

*** Fetcher

    The fetcher is an abstraction over the communicatioln of
    WikipediaBase with the outside world. It is a singleton object
    that implements a specific interface.

*** Infobox

*** Article

** Provider/Acquirer model

   WikipediaBase attempts to be modular and extendible. To accomplish
   this it is often useful to have parts of the system that access
   resources (eg. heuristic methods) without knowledge of what module
   those came from. Additionally it is often the case that resources
   come from many different modules. To avoid ad-hoc code and hard
   dependencies the provider / acqirer model was created:

   # XXX elaborate

   - Subclass provider/Acquirer classes
   - The Provider uses the =@provide= decorator to provide resources.
   - The acquirer has transparent access to the aggregate of provided
     values for a key.

   # XXX: example

** Testing
*** Unit testing

    The good functioning of WikipediaBase is assured by a
    comprehensive test suite of unit tests, functional tests and
    regression tests.

**** Unit tests

     Unit tests test small blocks of functionality, that are composed
     to create the system at large. For unit testing we use python's
     default testing library. Each test is a class the subclasses

**** Functional and regression tests

     Functional tests are tests written before, during or shortly
     after the development of a system and they assert the correct
     overall functioning of the system. Regression tests are very akin
     to functional tests. They prove that a found bug was fixed and
     assert that it will not appear again later. Functional and
     regression tests currently reside in =tests/examples.py=

*** Examples
** Synonyms
*** Good/Bad synonyms
*** Synonym generation
** Backend databases
*** DBM
*** SQLite
*** Redis
*** Postgres
** Data sources
** Date parser
   # Make this included http://orgmode.org/manual/Include-files.html

*** Parsing with overlays
    # TODO: Make this a bit clearer

    The concept of an overlay was inspired by emacs overlays. They are
    objects that specify the behavior of a subset of a text, by
    assigning properties to it. An overlay over a text \(t\) in our
    context is tuple of the range within that text, a set of tags that
    define semantic sets that the said substring is a member of, and
    arbitrary information (of type \(A\)) that the underlying text
    describes. More formally:

    #+BEGIN_EXPORT latex
    \begin{align*}
    & o_i \in TextRange\(t\) \times Set(Tag) \times A \\
    & Text \rightarrow \left\{o_1, o_2, ..., o_n\right\}
    \end{align*}
    #+END_EXPORT

    So for example out of the text

    #+BEGIN_EXPORT latex
    \[
    The weather today,
    \overbrace{Tuesday}^\text{\(o_1\)} \,
    \overbrace{21^{st}}^\text{\(o_2\)} \, of \,
    \overbrace{November}^\text{\(o_3\)} \,
    \overbrace{2016}^\text{\(o_4\)}, \, was \, sunny.
    \]
    #+END_EXPORT

    We can extract overlays \(\left\{o_1, ... , o_4\right\}\), so that

    #+BEGIN_EXPORT latex
    \[
    \begin{array}[b]{rlll}
    o_1 = (&r("Tuesday"),  & \{\mathrm{DayOfWeek}, \mathrm{FullName}\}, & 2) \\
    o_2 = (&r("21^{st}"),   & \{\mathrm{DayOfMonth}, \mathrm{Numeric}\}, & 21) \\
    o_3 = (&r("November"), & \{\mathrm{Month}, \mathrm{FullName} \}, & 11) \\
    o_4 = (&r("2016"),     & \{\mathrm{Year}, \mathrm{4digit} \}, & 2016)
    \end{array}
    \]
    #+END_EXPORT

    Notice how for all overlays of the example we have \(A =
    \mathbb{N}\), as we encode day of the week, day of the month,
    month and year as natural numbers. We encode more precise type
    information (ie that a day is inherently different than a month)
    in the tag set.

    Once we have a set of overlays we can define overlay sequences as
    overlays whose ranges are consecutive, that is their and their tag
    sets match particular patterns. For example we can search for
    sequences of overlays that match the pattern

    \[
    p = \mathrm{DayOfMonth}, \mathrm{Separator(/)}, (\mathrm{Month} \wedge \mathrm{Number}), \mathrm{Separator(/)}, \mathrm{Year}
    \]

    to match patterns like \(22/07/1991\), where \(Separator(/)\)
    matches only the character "/"

*** The implementation
*** Optimization
**** Comparison
*** The dates example
*** Benchmarks
** Future
*** Configuration
**** Persistence
**** Pass by reference
**** Lenses
**** Laziness
***** Referential (Ref - Items)
***** Computational
*** START deployment
*** Test suites
*** Bugs
*** Answer hierarchy
* WikipediaMirror

  # TODO: provide link

  wikipedia-mirror is a tool for generating mirrors of wikipedia.org
  using the dumps provided by wikipedia.org.

** Mediawiki stack

   Wikipedia-mirror builds upon the mediawiki stack provided by
   bitnami. A service that builds the entire server within the
   confines of a directory. This is useful because we avoided the
   overhead of dealing with container or VM technologies and we had
   direct access to the filesystem of the stack while still having
   bitnami's build system do the tedious job of orchestrating the
   various components and separating our sever from the rest of the
   system.

   The stack is comprised of

   - An http server, in our case apache
   - The web application runtime, in our case PHP
   - A database, in our cas MySQL
   - The web application itself, in our case mediawiki

   All of the above are provided by the the bitnami mediawiki stack.

   # TODO: more details about each element of the stack

*** Extensions

    For mediawiki to act like wikipedia a number of extensions are
    required. The installation process of such extensions is not
    automated or streamline. To automatically manage this complexity a
    mechanism is provided for declaratively installing extensions.  To
    add support for an extension to wikipediabase one needs to add the
    following code in =Makefile.mwextnesions= (modifying accordingly):

    #+BEGIN_SRC makefile
      MW_EXTENSIONS += newextension
      mw-newextension-url = url/to/new/extnesion/package.tar.gz
      mw-newextension-php = NewExtensionFile.php
      mw-newextension-config = '$$phpConfigVariable = "value";'
    #+END_SRC

    And wikipedia-mirror will take care of checking if the extension
    is already installed and if not it will put the right files in the
    right place and edit the appropriate configuration files. The
    entry points for managing extensions are (provided that the name
    of the registered extension is newextension):

    #+BEGIN_SRC sh
      make mw-print-registered-extensions # Output a list of the registed extensions
      make mw-newextension-enable         # Install and/or enable the extension
      make mw-newextension-reinstall      # Reinstall an extension
      make mw-newextension-disable        # Disable the extension
      make mw-newextension-clean          # Remove the extension
    #+END_SRC

    All registered extensions will be installed and enabled when
    wikipedia-mirror is built.

** Dumps

   # TODO: Provide links
   Wikipedia provides monthly dumps of all it's databases. The bulk of
   the dumps come in XML format and they need to be encoded into MySQL
   to be loaded into the wikipedia-mirror database. There are more
   than one ways to do that.

*** PHP script

    # TODO: provide link
    Mediawiki ships with a utility for importing the XML
    dumps. However it's use for importing a full blown wikipedia
    mirror is discouraged due to performance tradeoffs. Instead other
    tools like mwdumper are recommended that transform the XML dump
    into MySQL queries that populate the database.

*** mwdumper

    The recomended tool for translating the XML dumps into MySQL code
    is mwdumper. Mwdumper is written in java and is shipped separately
    from mediawiki.

**** Xml sanitizer

**** Article dropper

** Automation
*** Makefiles / laziness
*** Shell scripts
*** Bitnami
** Performance


*** Compile time

    Compile time includes the time it takes for:

    - Downloading all the components of a wikipedia server
      - The bitnami stack
      - mwdumper
      - mediawiki-extensions
    - Installing and building those components (~1 min)
    - Downloading the wikipedia dumps
    - Preprocessing the dumps (~10 mins)
    - Populating the mysql database (~10 days)

    # TODO: insert ashmore specifics
    Builds were done on Infolab's Ashmore. The system's specs are
    quite high end but the bottleneck was the disk IO so less than 1%
    of the rest of the available resources were used during the MySQL
    database population.

**** Attempts to optimizing MySQL

*** Runtime

    Runtime of wikipedia mirror turned out to be too slow to be useful
    and therefore the project was eventually abandoned. Namely for the
    full wikipedia dump of July 2014 the load time for the Barack
    Obama, not taking advantage of caching was at the order of ~30s.

* Related CSAIL projects
  # See github
