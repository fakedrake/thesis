#+TITLE:       Wikipedia Mirror
#+AUTHOR:      Chris Perivolaropoulos
#+DATE:        Wednesday 28 May 2016
#+EMAIL:       cperivol@csail.mit.edu
#+DESCRIPTION: Automated building of a local wikipedia mirror.
#+KEYWORDS:
#+LANGUAGE:    en
#+OPTIONS:     H:2 num:t toc:t \n:nil @:t ::t |:t ^:t f:t TeX:t
#+macro:       ref
#+STARTUP:     showall
#+MACRO:       ref

Wikipedia mirror is a system aiming to automate the creation of a
local clone of wikipedia containing only the articles - that is not
containing users, discussion and edit history. The automated process
includes setting up a server, a database and populating that database
with the wikipedia articles. The purpose for this is to provide the
option of accessing wikipedia's data set independently of
wikipedia.org.

* mediawiki stack overview

  Wikipedia-mirror builds upon the mediawiki stack provided by
  bitnami. A service that builds the entire server within the confines
  of a directory. This is useful because we avoided the overhead of
  dealing with container or VM technologies and we had direct access
  to the filesystem of the stack while still having bitnami's build
  system do the tedious job of orchestrating the various components
  and separating our server from the rest of the system.

  The stack is comprised of

  - An http server, in our case apache{{{ref(apache)}}}
  - The web application runtime, in our case PHP{{{ref(php)}}}
  - A database, in our case MySQL{{{ref(mysql)}}}
  - The web application itself, in our case mediawiki{{{ref(mediawiki)}}}

  All of the above are provided by the the bitnami mediawiki stack.
  Xampp{{{ref(xampp)}}} used to be go-to for that but it is
  unmaintained so we decided to go with bitnami which works pretty
  well.

  Once the stack is set up properly the wikipedia dump xml is
  downloaded and then turned into an sql dump with
  =mwdumper={{{ref}}}. It could be piped directly to MySQL but
  extracting can take time and things tend to go wrong during the
  dumping step.

** Elements of the stack

   We present each of the elements of the stack in more detail below.

*** Apache

    As per wikipedia{{{ref}}}:

    #+BEGIN_EXAMPLE
      The Apache HTTP Server, colloquially called Apache, is the world's
      most used web server software. Originally based on the NCSA HTTPd
      server, development of Apache began in early 1995 after work on the
      NCSA code stalled. Apache played a key role in the initial growth of
      the World Wide Web, quickly overtaking NCSA HTTPd as the dominant HTTP
      server, and has remained most popular since April 1996. In 2009, it
      became the first web server software to serve more than 100 million
      websites.

      Apache is developed and maintained by an open community of developers
      under the auspices of the Apache Software Foundation. Most commonly
      used on a Unix-like system (usually Linux), the software is available
      for a wide variety of operating systems besides Unix, including
      eComStation, Microsoft Windows, NetWare, OpenVMS, OS/2, and
      TPF. Released under the Apache License, Apache is free and open-source
      software.
    #+END_EXAMPLE

    It is fair to say that apache is at least one of the most popular
    web servers on the internet. wikipedia.org itself seems to be
    using a more complex stack involving [[https://en.wikipedia.org/wiki/Varnish_(software)][varnish]],{{{ref}}} an HTTP
    accelerator, and [[https://en.wikipedia.org/wiki/Nginx][nginx]],{{{ref(nginx)}}} an alternative, also quite
    popular HTTP server. We arrive at this conclusion by inspecting
    the headers returned by wikipedia.org. In the
    http://www.wikipedia.org case we are redirected to the secure
    domain (pay attention to the =Server:= line):

    #+BEGIN_SRC sh
      $ curl -s -D - http://www.wikipedia.org -o /dev/null
      HTTP/1.1 301 TLS Redirect
      Server: Varnish
      [...]
    #+END_SRC

    And if we directly ask for https://www.wikipedia.org nginx seems
    to be handling our request:

    However it is beyond the scope of the project to precisely
    replicate wikipedia's infrastructure. We focus on the
    functionality. Therefore due to the popularity, familiarity and by
    virtue of apache being part of the automatically installable
    bitnami mediawiki stack, we use it as our server.

*** PHP

    Mediawiki, which is discussed later, is written entirely in
    PHP, a popular server side, dynamically typed, object
    oriented scripting language. PHP is essential and is installed
    along the bitnami mediawiki stack. PHP is popular among web
    developers partly due to its support for multiple relational
    database libraries (including PostgreSQL, MySQL, Microsoft SQL
    Server and SQLite) and it essentially being structured as a
    template language generating HTML.

*** MySQL

    Mediawiki can use a number of different SQL database backends:

    - *MSSQL:* An SQL database by Microsoft{{{ref(mssql)}}}
    - *MySQL:* Using the standard PHP library for MySQL.
    - *MySQLi:* An extension to the MySQL backend{{{ref(mysqli)}}}.
    - *Oracle:* A propitiatory SQL database by Oracle{{{ref(oracle)}}}.
    - *SQLite:* An SQL database that is typically accessed as a
      library rather than over a client-server scheme as is the case
      with the other options on the list.

    Wikipedia provides multiple dump files for SQL tables of secondary
    importance in MySQL format (eg. page redirects, categories etc)
    and suggests =mwdumper= which parses the XML dumps of the
    wikipedia articles into MySQL. That and bitnami providing it as
    part of its automatically built stack, make MySQL the obvious
    choice for the wikipedia-mirror stack.

*** MediaWiki

    Mediawiki is the heart of wikipedia. MediaWiki is a free and
    open-source{{{ref(foss)}}} wiki application. It was originally
    developed by the Wikimedia Foundation{{{ref(wikimedia)}}} and runs
    on many websites, including Wikipedia,
    Wikitionary{{{ref(wikitionary)}}} and Wikimedia
    Commons{{{ref(wikimedia_commons)}}}. As mentioned previously, it
    is written in the PHP programming language and uses a backend
    database.

    The software has more than 800 configuration settings and more
    than 2,000 extensions available for enabling various features to
    be added or changed. On Wikipedia alone, more than 1000 automated
    and semi-automated bots and other tools have been developed to
    assist in editing. Most of this is not relevant for our
    purposes. The only extensions useful for our purposes are
    =scriunto= and =parserfunctions= and the only useful settings are
    related to the name of the site, the name of the database etc and
    are mostly handled by bitnami.

* Setting up

  Following are step by step instructions. First, clone the
  git{{{ref(git)}}} code repository:

  #+BEGIN_SRC sh
    $ git clone https://github.com/fakedrake/wikipedia-mirror
    $ cd wikipedia-mirror
  #+END_SRC

  At this point in theory one can run =make sql-load-dumps= which
  will take care of setting up everything needed to load the the
  database dumps into the working SQL database. Of course for that to
  happen first a couple of steps need to be carried out:

  - Download the wikipedia database dumps in XML format.
  - Transform them into a format that MySQL understands.
  - Set up the bitnami stack that includes a local install of MySQL
  - Load the MySQL dumps into MySQL

  All of these steps are encoded as part of the dependency hierarchy
  encoded into makefile targets and are in theory taken care of
  automatically, effectively yielding a functioning wikipedia
  mirror. However this process is extremely long and fragile so it is
  advised that each of these steps be run individually by hand.

  First, download and install bitnami. The following command will
  fetch an executable from the bitnami website and make a local
  installation of the bitnami stack discussed above:

  #+BEGIN_SRC sh
    $ make bmw-install
  #+END_SRC

  Next step is to make sure =maven=,{{{ref(maven)}}} the java is a
  software project management and comprehension is installed, required
  to install and setup mwdumper (see below). You can do that by making
  sure the following succeeds:

  #+BEGIN_SRC sh
    $ mvn --version
  #+END_SRC

  Note: if running on Ubuntu 14.04, you may need to install Maven
  (for Java) using =sudo apt-get install maven=.

  Now everything is installed to automatically download Wikipedia's
  XML dumps{{{ref(wikipedia_dumps)}}} and then convert them to SQL
  using mwdumper. First maven will be downloaded and built. Then the
  compressed XML dumps will be downloaded from the wikipedia, they
  will be uncompressed and finally converted to MySQL dumps using
  =mwdumper=. This is a fairly lengthy process taking 6 to 11 hours on
  a typical machine:

  #+BEGIN_SRC sh
    $ make sql-dump-parts
  #+END_SRC

  After that's done successfully you can load the SQL dumps to the
  MySQL database.

  #+BEGIN_SRC sh
    $ make sql-load-parts
  #+END_SRC

  Finally the

  #+BEGIN_SRC sh
    $ make mw-extensions
  #+END_SRC

** Installing mediawiki extensions

   For mediawiki to act like wikipedia a number of extensions are
   required. The installation process of such extensions is not
   automated or streamline. To automatically manage this complexity a
   mechanism is provided for declaratively installing extensions.  To
   add support for an extension to WikipediaBase one needs to add the
   following code in =Makefile.mwextnesions= (modifying accordingly):

   #+BEGIN_SRC makefile
     MW_EXTENSIONS += newextension
     mw-newextension-url = url/to/new/extnesion/package.tar.gz
     mw-newextension-php = NewExtensionFile.php
     mw-newextension-config = '$$phpConfigVariable = "value";'
   #+END_SRC

   And wikipedia-mirror will take care of checking if the extension
   is already installed and if not it will put the right files in the
   right place and edit the appropriate configuration files. The
   entry points for managing extensions are (provided that the name
   of the registered extension is =newextension=):

   #+BEGIN_SRC sh
     make mw-print-registered-extensions # Output a list of the registed extensions
     make mw-newextension-enable         # Install and/or enable the extension
     make mw-newextension-reinstall      # Reinstall an extension
     make mw-newextension-disable        # Disable the extension
     make mw-newextension-clean          # Remove the extension
   #+END_SRC

   All registered extensions will be installed and enabled when
   wikipedia-mirror is built.

** Loading mediawiki dumps

   Wikipedia provides monthly dumps of all its databases. The bulk of
   the dumps come in XML format and they need to be encoded into MySQL
   to be loaded into the wikipedia-mirror database. There are more
   than one ways to do that.

   Mediawiki ships with a utility for importing the XML
   dumps. However its use for importing a full blown wikipedia
   mirror is discouraged due to performance trade-offs. Instead other
   tools like mwdumper are recommended that transform the XML dump
   into MySQL queries that populate the database.

   However the recommended tool for translating the XML dumps into
   MySQL code is =mwdumper=. =mwdumper= is written in java and is shipped
   separately from mediawiki. Mwdumper can transform data between the
   following formats:

   - XML
   - MySQL dump
   - SQLite dump
   - CSV

   For our purpose we are only interested in the =XML -> MySQL dump=
   transformation.

* The xerces bug

  #+INCLUDE: xerces.org

* Tools

  A number of tools were developed in assisting the process of
  manipulating and monitoring the process of loading the dumps into
  the database. They are presented in details below. Since their
  source code is fairly concise it is presented in the Appendix.

*** sql-clear.sh

    =sql-clear.sh= is a small bash script that truncates all tables
    from a database. Truncating means leaving the MySQL table schemata
    unaffected and delete all internal data.

*** utf8thread.c

    =utf8thread.c= is another low level program that blanks out all
    invalid utf-8 characters from a file. We used =pthreads= to speed
    things up.

*** webmonitor.py

    =webmonitor.py= is a python script that sets up a web page that
    shows live data in the form of a histogram about the progress of
    the database population. =webmonitor.py= serves a static html page
    and then deeds it the data over websocket. Webmonitor can show any
    stream of =<epoc date> <float value>= pairs that it receives in
    its input. As a sample:

    #+BEGIN_SRC sh
    $ pip install tornado
    #+END_SRC

    First install the dependencies of the script. That would be
    =tornado=, an asynchronous web framework supporting websockets.
    Also use =tornado= to serve the following page:

    #+BEGIN_SRC html
      <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
      <html>
        <head>
          <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
          <title>DrNinjaBatmans Websockets</title>

          <script type="text/javascript" src="http://code.jquery.com/jquery-1.10.1.js"></script>
          <script type="text/javascript" src="http://code.highcharts.com/highcharts.js"></script>

          <script>
           var chart; // global
           var url = location.hostname + ':' + (parseInt(location.port));
           var ws = new WebSocket('ws://' + url + '/websocket');
           ws.onmessage = function(msg) {
               add_point(msg.data);
           };

           // ws.onclose = function() { alert('Connection closed.'); };

           var add_point = function(point) {
               var series = chart.series[0],
    	       shift = series.data.length > %d;

               chart.series[0].addPoint(eval(point), true, shift);
           };

           $(document).ready(function() {
               chart = new Highcharts.Chart(JSON.parse('%s'));
           });
          </script>

        </head>
        <body>
            <div id="container" style="width: 800px; height: 400px; margin: 0 auto"></div>
        </body>
      </html>
    #+END_SRC

    In essence this page expects to read a stream of values from a
    websocket at =ws://localhost:8888/hostname= -- although it is
    smart enough to change the =localhost:8888= if you are serving
    this to another location -- and plot them in real time using
    =highcharts.js=.

    The attentive reader may notice that the above is not quite HTML
    but rather a python formatted string. That is for two reasons:
    first because the chart configuration is handled by python rather
    than javascript, second because the width of the graph will is
    calculated at page load time -ie. by python- and the plot needs to
    be shifted to only show the most recent points.

    #+BEGIN_SRC sh
      $ for i in {1..100}; do echo $i;  sleep  1; done | \
          awk -oL "{print \$1/100}" | \
          python webmonitor.py
    #+END_SRC

    This will produce, in 1 second intervals, numbers from 1
    to 100. Then it normalizes them using =awk= and feeds them to
    webmonitor. After this command executes we can open the browser
    and then navigate to =localhost:8888=. We utilize this to remotely
    monitor the total size of data that =mysql= consumes.

*** xml-parse.sh

    Simply removing specific articles fixes the xerces error with
    UTF8. If the articles are alone the error goes away as well.  The
    =xml-parse.sh= script removes the requested article from the xml
    file.

    #+BEGIN_SRC sh
      xml-parse.sh <original-xml-file> <title_of_article_to_remove> [inplace]
    #+END_SRC

    if =inplace= is the last argument, the =page_remover.c= will be
    used to cover the article with spaces. This is much faster.
    Otherwise the page is just omitted and the result is dumped in
    =stdout=. After this script finishes you can run:

    #+BEGIN_SRC sh
      java -jar tools/mwdumper.jar RESULTING_XML --format=sql:1.5 > SQL_DUMP
    #+END_SRC

*** page\_remover.c

    As previously discussed, the =xerces= library that =mwdumper=
    depends on fails, seemingly at random, to process certain
    pages. To address this issue we remove the pages completely and
    retry. Since this task is fairly straight forward yet performance
    sensitive we resorted to writing a small low level program in C to
    address it, =page_remove.c=. Page remover accepts as input the
    path of the XML wikipedia dump, the offset of the article and the
    size of the article. It then uses the =mmap= system call to
    random-access the data within the file and fill the article with
    white space characters. =page_remover.c= is not threaded as the
    bottleneck is the HDD IO speed.


* Automation

  Creating a wikipedia mirror may seem like a straight forward task
  but it involves many caveats, nuances and repetitive tasks. Multiple
  methods of automation were employed to carry out the wide variety of
  tasks involved into the process.

** Makefiles / laziness

   #+INCLUDE: makefiles.org

** Bitnami

   Bitnami{{{ref(bitnami)}}} is a family of programs that sets up and
   manages servers stacks. It contains the entire stack installation
   within a directory making it both modular and portable while
   avoiding the fuss of dealing with VMs or containers. Bitnami is not
   open source so there is no way to tell for sure but my best guess
   is that it manages this by patching the prefix path of MySQL,
   apache etc binaries with the installation directory.

   Bitnami now supports hundreds of stacks, indicatively the most
   popular are:

   - Osclass
   - Joomla
   - Drupal
   - PrestaShop
   - MediaWiki
   - Moodle
   - ownCloud
   - Redmine
   - Wordpress

* Performance

** Compile time

   Compile time includes the time it takes for:

   - Downloading all the components of a wikipedia server
   - The bitnami stack
     - mwdumper
     - mediawiki-extensions
     - Installing and building those components (~1 min)
     - Downloading the wikipedia dumps
     - Pre-processing the dumps (~10 mins)
     - Populating the MySQL database (~10 days)

   Builds were done on InfoLab's ashmore. The system's specs are
   quite high end but the bottleneck was the disk IO so less than 1%
   of the rest of the available resources were used during the MySQL
   database population. The specifics of the ashmore machine are:

   - *CPU:* Xeon E5-1607 3GHz 4-Core 64 bit
   - *Main memory:* 64G
   - *HDD:* (spinning disk) 500GB + 2Tb

   Since the main bottleneck was the database population --- ie
   MySQL's performance --- great effort and experimentation went into
   fine tuning MySQL but the speedup achieved was negligible so they
   were not included in the makefiles.

   The backend database engine used by MySQL is InnoDB. Some of the
   optimization methods attempted are:

   - Calibrate the
     =innodb_buffer_pool_size={{{ref(innodb_buffer_pool_size)}}}. While
     the available memory in ashmore is fairly large, increasing the
     buffer pool size up to several GB there was no noticeable
     difference in database population.
   - Change =innodb_flush_method={{{ref(innodb_flush_method)}}} to
     =O_DSYNC= to avoid using the =fsync= system call. In short the
     problem with flushing large mapped files with
     =fsync={{{ref(fsync)}}} is that =fsync= searches for dirty pages
     in mapped memory pages linearly making it slower and slower as
     the file gets larger.
   - Calibrate the
     =innodb_io_capacity={{{ref(innodb_io_capacity)}}}. Unsurprisingly
     the value of this variable was higher than the bandwidth of the
     HDD.

  The only optimization that actually made a difference in database
  population speed was to edit the MySQL dump to set:

  #+BEGIN_SRC sql
    SET AUTOCOMMIT = 0; SET FOREIGN_KEY_CHECKS=0;
  #+END_SRC

  This allowed InnoDB to do more work in the main memory before
  committing to the disk and also reduced the overall work by trusting
  that the keys indicating relation to the database actually point
  somewhere.

** Runtime

   Runtime of wikipedia mirror turned out to be too slow to be useful
   and therefore the project was eventually abandoned. Namely for the
   full wikipedia dump of July 2014 the load time for the Barack
   Obama, not taking advantage of caching was at the order of ~30s.

* Appendix

** script sources

   #+INCLUDE: sources.org
