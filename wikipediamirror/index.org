#+TITLE:       Wikipedia Mirror
#+AUTHOR:      Chris Perivolaropoulos
#+DATE:        Sunday 21 February 2016
#+EMAIL:       cperivol@csail.mit.edu
#+DESCRIPTION: Automated building of a local wikipedia mirror.
#+KEYWORDS:
#+LANGUAGE:    en
#+OPTIONS:     H:2 num:t toc:t \n:nil @:t ::t |:t ^:t f:t TeX:t
#+STARTUP:     showall


* The xerces bug

  #+INCLUDE: xerces.org

* mediawiki stack overview

  Wikipedia-mirror builds upon the mediawiki stack provided by
  bitnami. A service that builds the entire server within the
  confines of a directory. This is useful because we avoided the
  overhead of dealing with container or VM technologies and we had
  direct access to the filesystem of the stack while still having
  bitnami's build system do the tedious job of orchestrating the
  various components and separating our sever from the rest of the
  system.

  The stack is comprised of

  - An http server, in our case apache
  - The web application runtime, in our case PHP
  - A database, in our cas MySQL
  - The web application itself, in our case mediawiki

  All of the above are provided by the the bitnami mediawiki stack.
  Xampp used to be go-to for that but it is unmaintained so we decided
  to go with bitnami which works pretty well.

  Once the stack is set up properly the wikipedia dump xml is
  downloaded and then turned into an sql dump with mwdumper. Could be
  piped directly to MySQL? but extracting can take time and things
  tend to go wrong during the dumping step.

** Elements of the stack
   # copy the shit out of wikipedia

   We present each of the elements of the stack in more detail below.

*** Apache

    As per wikipedia:

    #+BEGIN_EXAMPLE
      The Apache HTTP Server, colloquially called Apache, is the world's
      most used web server software. Originally based on the NCSA HTTPd
      server, development of Apache began in early 1995 after work on the
      NCSA code stalled. Apache played a key role in the initial growth of
      the World Wide Web, quickly overtaking NCSA HTTPd as the dominant HTTP
      server, and has remained most popular since April 1996. In 2009, it
      became the first web server software to serve more than 100 million
      websites.

      Apache is developed and maintained by an open community of developers
      under the auspices of the Apache Software Foundation. Most commonly
      used on a Unix-like system (usually Linux), the software is available
      for a wide variety of operating systems besides Unix, including
      eComStation, Microsoft Windows, NetWare, OpenVMS, OS/2, and
      TPF. Released under the Apache License, Apache is free and open-source
      software.
    #+END_EXAMPLE

    it is fair to say that apache is at least one of the most popular
    web servers on the internet. wikipedia.org itself seems to be
    using a more complex stack involving [[https://en.wikipedia.org/wiki/Varnish_(software)][varnish]], an HTTP accelerator,
    and [[https://en.wikipedia.org/wiki/Nginx][nginx]], an alternative, also quite popular HTTP server. We
    arrive at this conclusion by inspecting the headers returned by
    wikipedia.org. In the http://www.wikipedia.org case we are
    redirected to the secure domain (pay attention to the =Server:=
    line):

    #+BEGIN_SRC sh
      $ curl -s -D - http://www.wikipedia.org -o /dev/null
      HTTP/1.1 301 TLS Redirect
      Server: Varnish
      [...]
    #+END_SRC

    And if we directly ask for https://www.wikipedia.org nginx seems
    to be handling our request:

    #+BEGIN_SRC sh
      $ curl -s -D - https://www.wikipedia.org -o /dev/null
      HTTP/1.1 200 OK
      Server: nginx/1.9.4
      [...]
    #+END_SRC

    However it is beyond the scope of the project to precisely
    replicate wikipedia's infrastructure. We focus on the
    functionality. Therefore due to the popularity, familiarity and by
    virtue of apace being part of the automatically installable
    bitnami mediawiki stack, we use it as our server.

*** PHP

    Mediawiki, which is discussed later, is written entirely in PHP, a
    popular server side, dynamically typed, object oriented scripting
    language. PHP is essential and is installed along the bitnami
    mediawiki stack. PHP is popular among web developers partly due to
    it's support for multiple relational database libraries (including
    PostgreSQL, MySQL, Microsoft SQL Server and SQLite) and it
    essentially being structred as a template language generating
    HTML.

*** MySQL

    Mediawiki can use a number of different SQL database backends:

    - *MSSQL:* An SQL database by Microsoft
    - *MySQL:* Using the standard PHP library for MySQL.
    - *MySQLi:* An extension to the MySQL backend
    - *Oracle:* A propertiary SQL database by Oracle.
    - *SQLite:* An SQL database that is typically accessed as a
      library rather than over a client-server scheme as is the case
      with the other options on the list.

    Wikipedia provides multiple dump files for SQL tables of secondary
    importance in MySQL format (eg. page redirects, categories etc)
    and suggests =mwdumper= which parses the XML dumpls of the
    wikipedia articles into MySQL. That and bitnami providing it as
    part of it's automatically built stack, make MySQL the obvious
    choice for the wikipedia-mirror stack.

*** MediaWiki

    Mediawiki is the heart of wikipedia.

** Tools

   A number of tools were developed in assisting the

*** page_remover.c

    As previously discussed, the =xerces= library that =mwdumper=
    depends on fails, seemingly at random, to process certain
    pages. To address this issue we remove the pages completely and
    retry. Since this task is fairly straight forward yet performance
    sensitive we resorted to writing a small low level program in C to
    address it, =page_remove.c=. Page remover accepts as input the
    path of the XML wikipedia dump, the offset of the article and the
    size of the article. It then uses the =mmap= system call to
    random-access the data within the file and fill the article with
    withespace characters. =page_remover.c= is not threaded as the
    bottleneck is the HDD IO speed.

*** sql-clear.sh

    =sql-clear.sh= is a small bash script that truncates all tables
    from a database. Truncating means leaving the table scheamata
    unaffected and delete all internal data.

*** utf8thread.c

    =utf8thread.c= is another low level program that blanks out all
    invalid utf-8 characters from a file. We used =pthreads= to speed
    things up.

*** webmonitor.py

    =webmonitor.py= is a python script that sets up a web page that
    shows live data in the form of a histogram about the progress of
    the database population. =webmonitor.py= serves a static html page
    and then deeds it the data over websocket. Webmonitor can show any
    stream of =<epoc date> <float value>= pairs that it receives in
    it's input. As a sample:

    #+BEGIN_SRC sh
    $ pip install tornado
    #+END_SRC

    First install the dependencies of the script. That would be
    tornado, an asynchronous web framework supporting websockets. We
    will instruct =tornado= tornado will serve the following page:

    #+BEGIN_SRC html
      <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
      <html>
        <head>
          <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
          <title>DrNinjaBatmans Websockets</title>

          <script type="text/javascript" src="http://code.jquery.com/jquery-1.10.1.js"></script>
          <script type="text/javascript" src="http://code.highcharts.com/highcharts.js"></script>

          <script>
           var chart; // global
           var url = location.hostname + ':' + (parseInt(location.port));
           var ws = new WebSocket('ws://' + url + '/websocket');
           ws.onmessage = function(msg) {
               add_point(msg.data);
           };

           // ws.onclose = function() { alert('Connection closed.'); };

           var add_point = function(point) {
               var series = chart.series[0],
    	       shift = series.data.length > %d;

               chart.series[0].addPoint(eval(point), true, shift);
           };

           $(document).ready(function() {
               chart = new Highcharts.Chart(JSON.parse('%s'));
           });
          </script>

        </head>
        <body>
            <div id="container" style="width: 800px; height: 400px; margin: 0 auto"></div>
        </body>
      </html>
    #+END_SRC

    In essence this page expects to read a stream of values from a
    websocket at =ws://localhost:8888/hostname= -- although it is
    smart enough to change the =localhost:8888= if you are serving
    this to another location -- and plot them in real time using
    =highcharts.js=.

    The attentive reader may notice that the above is not quite HTML
    but rather a python formatted string. That is for two
    reasones. First because the script handles the configuration (see
    =chart = new Highcharts.Chart(JSON.parse('%s'));=). Second because
    the width of the graph will is calulated at page load time and the
    plot needs to be shifted to only show the most recent points.

    #+BEGIN_SRC sh
      $ for i in {1..100}; do echo $i;  sleep  1; done | \
          awk -oL "{print \$1/100}" | \
          python webmonitor.py
    #+END_SRC

    This will produce, in 1 second intervals, numbers from 1
    to 100. Then it normalizes them using =awk= and feeds them to
    webmonitor. After this command executes we can open the browser
    and then navigate to =localhost:8888=.

    # XXX: Screenshot

    We utilize this to remotely monitor the total size of data that
    =mysql= consumes.

*** xml-parse.sh

** Setting up

   Following are step by step instructions First, clone the git repo:

   #+BEGIN_SRC sh
     $ git clone https://github.com/fakedrake/wikipedia-mirror
     $ cd wikipedia-mirror
   #+END_SRC

   At this point in theory one can run =make sql-load-dumps= which
   will take care of stting up everything needed to load the the
   database dumps into the working SQL database. Of course for that to
   happen first a couple of steps need to be carried out:

   - Download the wikipedia database dumps in XML format.
   - Transform them into a format that MySQL understands.
   - Set up the bitnami stack that includes a local install of MySQL
   - Load the MySQL dumps into MySQL

   All of these steps are encoded as part of the a dependency
   hierarchy encoded into makefile targets and are in theory taken
   care of automatically, effectively yielding a functioning wikipedia
   mirror. However this process is extremely long fragile so it is
   advised that each of these steps be run individually by hand.

   First, download and install bitnami. The following command will
   fetch an executable from the bitnami website and make a local
   installation of the bitnami stack discussed above:

   #+BEGIN_SRC sh
     $ make bmw-install
   #+END_SRC

   Next step is to make sure =maven=, the java is a software project
   management and comprehension is installed, required to install and
   setup mwdumper (see below). You can do that by making sure the
   following succeeds:

   #+BEGIN_SRC text
     $ mvn --version
   #+END_SRC

   Note: if running on Ubuntu 14.04, you may need to install Maven
   (for Java) using =sudo apt-get install maven=.

   Now everything is installed to automatically download Wikipedia's
   XML dumps and then convert them to SQL using maven. First maven
   will be downloaded and built. Then the compressed XML dumps will be
   downloaded from the wikipedia, they will be uncompressed and
   finally converted to MySQL dumps using =mwdumper=. This is a fairly
   lengthy process taking 6 to 11 hours on a typical machine:

   #+BEGIN_SRC sh
     $ make sql-dump-parts
   #+END_SRC

   After that's done successfully you can load the SQL dumps to the
   MySQL database.

   #+BEGIN_SRC sh
     $ make sql-load-parts
   #+END_SRC

   Finally the

   #+BEGIN_SRC sh
     $ make mw-extensions
   #+END_SRC

* Mediawiki Extensions

  For mediawiki to act like wikipedia a number of extensions are
  required. The installation process of such extensions is not
  automated or streamline. To automatically manage this complexity a
  mechanism is provided for declaratively installing extensions.  To
  add support for an extension to wikipediabase one needs to add the
  following code in =Makefile.mwextnesions= (modifying accordingly):

  #+BEGIN_SRC makefile
    MW_EXTENSIONS += newextension
    mw-newextension-url = url/to/new/extnesion/package.tar.gz
    mw-newextension-php = NewExtensionFile.php
    mw-newextension-config = '$$phpConfigVariable = "value";'
  #+END_SRC

  And wikipedia-mirror will take care of checking if the extension
  is already installed and if not it will put the right files in the
  right place and edit the appropriate configuration files. The
  entry points for managing extensions are (provided that the name
  of the registered extension is newextension):

  #+BEGIN_SRC sh
    make mw-print-registered-extensions # Output a list of the registed extensions
    make mw-newextension-enable         # Install and/or enable the extension
    make mw-newextension-reinstall      # Reinstall an extension
    make mw-newextension-disable        # Disable the extension
    make mw-newextension-clean          # Remove the extension
  #+END_SRC

  All registered extensions will be installed and enabled when
  wikipedia-mirror is built.

* Dumps

  Wikipedia provides monthly dumps of all it's databases. The bulk of
  the dumps come in XML format and they need to be encoded into MySQL
  to be loaded into the wikipedia-mirror database. There are more
  than one ways to do that.

** PHP script

   # TODO: provide link
   Mediawiki ships with a utility for importing the XML
   dumps. However it's use for importing a full blown wikipedia
   mirror is discouraged due to performance tradeoffs. Instead other
   tools like mwdumper are recommended that transform the XML dump
   into MySQL queries that populate the database.

** mwdumper

   The recomended tool for translating the XML dumps into MySQL code
   is mwdumper. Mwdumper is written in java and is shipped separately
   from mediawiki.

*** Xml sanitizer

*** Article dropper
* Automation

  Creating a wikipedia mirror may seem like a straight forward task
  but it involves many caveats, nuances and repetitive tasks. Multiple
  methods of autmoation were employed to carry out the wide variety of
  tasks involved into the process.

** Makefiles / laziness

   The most important part of wikipedia-mirror automation is the
   =make= build system. Make is a build system whereby one can declare
   required files (targets), dependencies for them, and a set of shell
   commands that will build those targets. Each target is essentially
   a finite state machine with two states:

   - A file that exists and is up to date with it's dependencies and
   - A file that either doesn't exist or it's modification date is
     older than that of at leas one of it's dependencies.

   And a sequence of shell commands to transition from the latter to
   the former state.

   For example, save the following as =Makefile= in a project that
   contains the files =foo.c=, =foo.h=, =bar.c= and =bar.h=:

   #+BEGIN_SRC makefile
     foo.o: foo.c foo.h
   	  gcc foo.c -c -o foo.o

     bar.o: bar.c
         gcc bar.c -c -o bar.o

     foobar: foo.o bar.o
   	  gcc foo.o bar.o -o foobar
   #+END_SRC

   this means that to build =foobar= we need =foo.o= and =bar.o=. And
   to build =foo.o= and =bar.o= we need =foo.c= and =foo.h=, and
   =bar.c= and =bar.h= respectively. We also provide commands for
   building =foo.o=, =bar.o= and =foobar=, which are

   - =gcc foo.c -c -o foo.o=
   - =gcc bar.c -c -o bar.o=
   - and =gcc foo.o bar.o -o foobar=

   respectively. Notice that there are no rules for the =.c= and =.h=
   files. That is because =make= should fail if they are not
   present. So if we run =make foobar=, make will check for =foobar='s
   existence and modification date. If foobar is missing or it's
   modification date is earlier than it's deopendecies' (ie =foo.o=
   and =bar.o=) it will be rebuilt. If any dependecies are missing the
   same logic is applied to that. This way if we build foobar once,
   and then edit =bar.c= and rerun =make foobar=, =make= will
   recursively deduce that

   - =bar.o= is out of date with respect to it's dependency =bar.c=
   - When =bar.o= is rebuilt it now has a more recent modification
     date than =foobar= and therefore the latter is out of date with
     respect to it's dependency =bar.o= so it needs to be rebuilt.

   This way make can infer a near optimal strategy for building each
   time the minimum amount of required targets.

   Now that we made the basic logic of =make= clear let's dive into
   some basic features that make our life easier.

*** Phony targets

    Some tasks do not result in a file and thay need to be run every
    time =make= encounters them in the dependency tree. For this we
    have the special keywork =.PHONY:=. Here is an example.

    #+BEGIN_SRC makefile
      .PHONY:
      clean:
    	  rm -rf *
    #+END_SRC

    This tells make that no file named =clean= will emerge from
    running =rm -rf *=, and also that even if an up-to-date file named
    =clean= exists, this target is to be run regardless.

    It is worth noting that phony dependencies will always render the
    dependent target out-of-date. For example:

    #+BEGIN_SRC makefile
      .PHONY:
      say-hello:
    	  echo "hello"

      test.txt: say-hello
    	  touch test.txt
    #+END_SRC

    When =touch test.txt= will be run *every* time we run =make
    test.txt= simply becaus make can not be sure that the phony target
    =say-hello= did not change anything important for =test.txt=. For
    this reason phony targets are only meant for user facing tasks.

*** Variables

    =makefiles= can have variables defined in a variety of ways. Some
    cases that are being made use of in wikiepedia-mirror are
    presented below.


**** COMMENT Recursively expanded variables

     #+BEGIN_SRC makefile
      OBJCETS = foo.o bar.o

      show:
          echo $(OBJECTS)
     #+END_SRC

     Running =make show= wil print =foo.o bar.o= to the console. All
     variables are substituted for their value by wrapping the
     variable name in parentheses and prefixing the dolear sign
     (=$=). Makefile variables have no type, reference of the
     variables is equivalent to simpe string substitution, much like
     it is in unix shell scripting.

     Variabales defined using a simple equal =\== sign are recursively
     expanded. This means that after the variable name is substituted
     for the variable content a recursive process keeps expanding
     emergent variables. This can make variable expansion a very
     powerful tool. For example:

     #+BEGIN_SRC makefile
       library = foo

       foo-libs = -lfoo
       foo-includes = -I./include/foo

       bar-libs = -lbar
       bar-includes = -I./include/bar

       libs = $($(library)-libs)
       includes = $($(library)-includes)

       waz:
     	  gcc waz.c $(includes) $(libs)
     #+END_SRC

     To demonstrate:

     #+BEGIN_SRC sh
       $ make --just-print
       gcc waz.c -I./include/foo -lfoo
     #+END_SRC

     The expansion that took place step by step are

     #+BEGIN_SRC makefile
       gcc waz.c $(includes) $(libs)
       gcc waz.c $($(library)-includes) $($(library)-libs)
       gcc waz.c $(foo-includes) $(foo-libs)
       gcc waz.c -I./include/foo -lfoo
     #+END_SRC

     Notice how variable names were themselves constructed.

     Variables can also be defined at the command so in this
     particular example we could easily switch to the =bar= library:

     #+BEGIN_SRC sh
       $ make --just-print library=bar
       gcc waz.c -I./include/bar -lbar
     #+END_SRC

**** Simple variables

     Sometimes it is not desirable for variables to be expanded
     indefinitely:

     #+BEGIN_SRC makefile
       kurma = the world $(support1)
       animal1 = four elephants
       animal2 = tortoise
       support1 = supported by $(animal1) $(support2)
       support2 = supported by a $(animal2) $(support2)

       all:
     	  echo $(kurma)
     #+END_SRC

     So what would makefile have said to Bertrand Russell [refrence] (or
     whoever)?

     #+BEGIN_SRC sh
       $ make --just-print
       Makefile:5: *** Recursive variable `support2' references itself (eventually).  Stop.
     #+END_SRC

     So in a way the variable system of make is total [reference]. As
     the hindus "solved" this problem so can we in makefiles:

     #+BEGIN_SRC makefile
       kurma = the world $(support1)
       animal1 = four elephants
       animal2 = tortoise
       support1 = supported by $(animal1) $(support2)
       support2 := supported by a $(animal2) $(support2)

       all:
          echo $(kurma)
     #+END_SRC

     And when we run make we get:

     #+BEGIN_SRC sh
       make --just-print
       echo the world supported by four elephants supported by a tortoise
     #+END_SRC

     So basically =support2= is removed from scope when the =support2=
     variable is substituted.

**** Automatic variables

     Makefile also defines some contextual variables that are
     defined. The automatic variables defined by gnu make are the
     following

     - =$@=: The file name of the target of the rule. If the target is
       an archive member, then =$@= is the name of the archive
       file. In a pattern rule that has multiple targets (see
       Introduction to Pattern Rules), =$@= is the name of
       whichever target caused the rule's recipe to be run.
     - =$%=: The target member name, when the target is an archive
       member. See Archives. For example, if the target is
       foo.a(bar.o) then =%%= is bar.o and =$@= is
       foo.a. =$%= is empty when the target is not an archive member.
     - =$<=: The name of the first prerequisite. If the target got its
       recipe from an implicit rule, this will be the first prerequisite
       added by the implicit rule (see Implicit Rules).
     - =$?=: The names of all the prerequisites that are newer than
       the target, with spaces between them. For prerequisites which
       are archive members, only the named member is used (see
       Archives).
     - =$^=: The names of all the prerequisites, with spaces between
       them. For prerequisites which are archive members, only the
       named member is used (see Archives). A target has only one
       prerequisite on each other file it depends on, no matter how
       many times each file is listed as a prerequisite. So if you
       list a prerequisite more than once for a target, the value of
       $^ contains just one copy of the name. This list does not
       contain any of the order-only prerequisites; for those see the
       =$|= variable, below.
     - =$+=: This is like =$^=, but prerequisites listed more
       than once are duplicated in the order they were listed in the
       makefile. This is primarily useful for use in linking commands
       where it is meaningful to repeat library file names in a
       particular order.
     - =$|= The names of all the order-only prerequisites, with spaces
       between them.
     - =$*=: The stem with which an implicit rule matches (see How
       Patterns Match). If the target is dir/a.foo.b and the target
       pattern is a.%.b then the stem is dir/foo. The stem is useful
       for constructing names of related files. In a static pattern
       rule, the stem is part of the file name that matched the =%= in
       the target pattern. In an explicit rule, there is no stem; so
       =$*= cannot be determined in that way. Instead, if the target
       name ends with a recognized suffix (see Old-Fashioned Suffix
       Rules), =$*= is set to the target name minus the suffix. For
       example, if the target name is =foo.c=, then =$*= is set to
       =foo=, since =.c= is a suffix. GNU make does this bizarre thing
       only for compatibility with other implementations of make. You
       should generally avoid using =$*= except in implicit rules or
       static pattern rules. If the target name in an explicit rule
       does not end with a recognized suffix, =$*= is set to the empty
       string for that rule.


*** Functions


** Bitnami
* Performance

** Compile time

   Compile time includes the time it takes for:

   - Downloading all the components of a wikipedia server
   - The bitnami stack
     - mwdumper
     - mediawiki-extensions
     - Installing and building those components (~1 min)
     - Downloading the wikipedia dumps
     - Preprocessing the dumps (~10 mins)
     - Populating the mysql database (~10 days)

       # TODO: insert ashmore specifics
       Builds were done on Infolab's Ashmore. The system's specs are
       quite high end but the bottleneck was the disk IO so less than 1%
       of the rest of the available resources were used during the MySQL
       database population.

*** Attempts to optimizing MySQL

** Runtime

   Runtime of wikipedia mirror turned out to be too slow to be useful
   and therefore the project was eventually abandoned. Namely for the
   full wikipedia dump of July 2014 the load time for the Barack
   Obama, not taking advantage of caching was at the order of ~30s.

* Appendix (script sources)

  #+INCLUDE: sources.org
