#+TITLE:       Extracting relational data from Wikipedia
#+AUTHOR:      Chris Perivolaropoulos
#+DATE:        Tuesday 9q May 2016
#+EMAIL:       cperivol@csail.mit.edu
#+DESCRIPTION: Making sense of semi structured data in wikipedia.
#+KEYWORDS:
#+LATEX_CLASS: report
#+LANGUAGE:    en
#+OPTIONS:     H:2 num:t toc:t \n:nil @:t ::t |:t ^:t f:t TeX:t
#+STARTUP:     showall
#+MACRO:       ref
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \setmainfont{Times}

* Περίληψη

  MiT InfoLab's START (SynTactic Analysis using Reversible Transforma-
  tions) είναι το πρώτο διαδικτυακό σύστημα παγκοσμίως που βασίζεται
  σε ερώτηση και απάντηση. Για την πρόσβαση σε περισσότερες πηγές
  δεδομένων χρησιμοποιεί το Omnibase, μια "εικονική βάση δεδομένων"
  που παρέχει πρόσβαση σε πολλαπλές πηγές στο διαδίκτυο . Αναπτύξαμε
  Wikipedia-Base για την παροχή παρόμοιου τρόπου δεδομένων όπως το
  Omnibase για το START. Με στόχο να αποκτηθεί για να αποκτηθεί
  πρόσβαση σε μη δομημένες και ημιδομημένες πληροφορίες στη
  Wikipedia. Στο πλαίσιο αυτού του στόχου επίσης δημιουργήσαμε το
  wikipedia-mirror, ένα πρόγραμμα που δημιουργεί κλώνους της Wikipedia
  που μπορεί να τρέχουν τοπικά, με αποτέέσμα να παρέχει έλεγχο και
  απεριόριστη πρόσβαση στο σύνολο των δεδομένων της wikpedia χωρίς να
  εξαρτάται ή να καταχράται το wikipedia.org.

  MiT InfoLab's START (SynTactic Analysis using Reversible
  Transformations) is the world's first web-based question answering
  system. For accessing most data sources it takes advantage of
  Omnibase, the /"virtual database"/ providing uniform access to
  multiple sources on the web. We developed WikipediaBase to provide
  an Omnibase-like way for START to access unstructured and
  semi-structured information in Wikipedia. As part of this goal we
  also created wikipedia-mirror, a program to create Wikipedia clones
  that can be run locally, to provide control and unrestricted access
  to the wikipedia data set without depending on or abusing
  wikipedia.org.

* Εισαγωγή

** START

   The START Natural Language System είναι ένα σύστημα λογισμικού που
   έχει σχεδιαστεί για να απαντά σε ερωτήσεις που τίθενται σε αυτό σε
   φυσική γλώσσα. Το START σαρώνει τις εισερχόμενες ερωτήσεις, τις
   ταυτίζει με τα ερωτήματα που δημιουργούνται από τα parse trees, τις
   αναλύει σύμφωνα με την γνωστική του βάση και τελικά παρουσιάζει τα
   κατάλληλα τμήματα πληροφοριών για το χρήστη. Με τον τρόπο αυτό , το
   START παρέχει στους ανεκπαίδευτους χρήστες γρήγορη πρόσβαση σε γνώση
   που σε πολλές περιπτώσεις μπορεί να είναι χρονοβόρα ακόμα και για
   τον ειδικό να την αποκτήσει

   Το START (SynTactic Analysis using Reversible Transformations)
   δημιουργήθηκε από τον Dr Boris Katz στο Artificial Intelligence
   Laboratory του MIΤ. Επί του παρόντος, το σύστημα υφίσταται περαιτέρω
   ανάπτυξη από το InfoLab Group, με επικεφαλής τον Dr Boris Katz. Το
   START για πρώτη φορά συνδέθηκε με το World Wide Web το Δεκέμβριο του
   199 , και με τις διάφορες μορφές του έχει μέχρι σήμερα απαντήσει σε
   εκατομμύρια ερωτήσεις από χρήστες σε όλο τον κόσμο.

   Μια βασική τεχνική που ονομάζεται "natural language annotation"
   βοηθά το START να συνδέσει την αναζητούμενη πληροφορία με τις πηγές
   πληροφόρησης. Αυτή η τεχνική χρησιμοποιεί προτάσεις και φράσεις της
   φυσικής γλώσσας ως περιγραφές περιεχομένων που σχετίζονται με τα
   τμήματα πληροφοριών σε διάφορες υποδιαιρέσεις. Ένα τμήμα των
   πληροφοριών ανακτάται όταν ο σχολιασμός του ταιριάζει με την
   εισαγόμενη ερώτηση. Αυτή η μέθοδος επιτρέπει στο START να χειριστεί
   μεγάλη ποικιλία μέσων, συμπεριλαμβανομένων των κειμένων,
   διαγραμμάτων, εικόνων , βίντεο και ήχων, συνόλων δεδομένων,
   ιστοσελίδων και άλλων.

   Η επεξεργασία της φυσικής γλώσσας του START αποτελείται από δύο
   ενότητες που μοιράζονται την ίδια γραμματική. Η μονάδα κατανόησης
   αναλύει το αγγλικό κείμενο και παράγει μια γνωστική βάση που
   κωδικοποιεί πληροφορίες που βρίσκονται στο κείμενο. Λαμβάνοντας
   υπόψη ένα κατάλληλο τμήμα της βάσης δεδομένων, η μονάδα παραγωγής
   γλώσσας παράγει αγγλικές προτάσεις . Αυτές οι ενότητες σε συνδυασμό
   με την τεχνική της φυσικής γλώσσας σχολιασμού, δίνουν την δυνατότητα
   της παραγωγής φυσικής γλώσσας σε επίπεδο προτάσεων με χρήση σε
   υπηρεσίες πρόσβασης πληροφοριών διαφόρων πολυμέσων.

** Omnibase, wikipedia, WikipediaBase και Wikipedia-Mirror

   Omnibase είναι μια "εικονική" βάση δεδομένων που παρέχει μια ενιαία
   διεπαφή για πολλαπλές πηγές γνώσης στο Web , ικανή να εκτελεί τα
   δομημένα ερωτήματα που παράγονται από το START. Το Omnibase
   αναπτύχθηκε για πρώτη φορά το 2002 , περίπου το ίδιο χρονικό
   διάστημα που η wikipedia έκανε την πρώτη της εμφάνιση (2001).

   Η διαδικτυακή εγκυκλοπαίδεια Wikipedia είναι μια τεράστια, συνεχώς
   εξελισσόμενο δίκτυο από πλούσιες αλληλένδετες πληροφορίες σε μορφή
   κειμένου. Στην αυξανόμενη κοινότητα των νέων ερευνητών και
   προγραμματιστών είναι μια συνεχώς εξελισσόμενη πηγή χειροκίνητα
   οριζόμενων εννοιών και σημασιολογικών σχέσεω. Αποτελεί έναν
   απαράμιλλο και σε μεγάλο βαθμό ανεκμετάλλευτο πόρο για την
   επεξεργασία φυσικής γλώσσας προγραμματισμού, διαχείριση της γνώσης,
   εξόρυξη δεδομένων, και διάφορους άλλους τομείς έρευνας. Είναι το
   προϊόν της συνεργασίαςεργασία εκατομμυρίων ανθρώπων. Η Wikipedia
   βασίζεται στο σύστημα wiki, μια κατηγορία ιστοσελίδων που
   επιτρέπουν την συνεργατική τροποποίηση του περιεχομένου.

   Λόγω της πολυπλοκότητας και την ιδιαίτερα αδόμητη φύση της
   wikipedia αντί του omnibase backend START χρησιμοποιήσαμε μια
   ξεχωριστή υπηρεσία , το WikipediaBase, που αποτελεί και το
   αντικείμενο της παρούσας διατριβής. Επίσης, για να αποφευχθεί ο
   κορεσμός του wikiedpedia.org χρησιμοποιήσαμε το wikipedia - mirror
   δημιουργώντας έναν κλώνο της wikipedia για την WikipediaBase
   χρησιμοποιώ.

* Wikipediabase

  Η WikipediaBase είναι ένα backend για το START υπεύθυνη για την
  παροχή πρόσωασης σε πληροφορίες που σχετίζονται με την
  wikipedia. Μιμείται την διεπαφή API πουπροέρχεται από το Omnibase. Η
  Wikipediabase έχει ξαναγραφεί δυο φορές. Η αρχική έκδοση ήταν
  γραμμένη σε Java. Στη συνέχεια ξαναγραφτηκε σε Ruby διατηρώντας την
  αρχική αρχιτεκτονική και το σχεδιασμό και τώρα ξαναγράφεται σε
  python με νέο σχεδιασμό και αρχιτεκτονική.

  Υπάρχουν δύο βασικοί λόγοι για αυτό:Η Python διδάσκεται ως
  προπτυχιακό και μεταπτυχιακό μάθημα στο MIT , και ως εκ τούτου, μια
  βάση κώδικα σε Python θα κάνει την έναρξη των νέων φοιτητών του ΜΙΤ
  ομαλότερη. Το πιο σημαντικό όμως είναι ότι ενώ ο αρχικός σχεδιασμός
  του προηγούμενου WikipediaBase θα έπρεπε να ήταν επαρκής μεγάλωσε σε
  τέτοιο σημείο όπου ο κώδικας ήταν ad-hoc και δύσκολα να κατανοηθεί,
  λόγος για την επέκταση

  Η εφαρμογή python αρχικά γράφτηκε από τον Χρήστο Περιβολαρόπουλο σε
  στενή συνεργασία με την Dr Sue Felshin και τελικά παραδόθηκε στους
  Sue Felshin , Alvaro Morales και ton Michael Silver. Αργότερα και
  άλλοι φοιτητές έχουν ενταχθεί στο έργο.

** Λειτουργικότητα

   Στη WikipediaBase , το καθέ (υποστηριζόμενο) Wikipedia infobox
   ορίζεται ως class, και κάθε μεταβλητή στο infobox ορίζεται ως ένα
   χαρακτηριστικό της κάθε class. Όλα τα αντικείμενα της WikipediaBase
   ανήκουν κληρονομικά στην υπερκλάση wikibase-term, η οποία
   υποστηρίζει τα χαρακτηριστικά =IMAGE-DATA=, =SHORT-ARTICLE=, =URL=,
   =COORDINATES=, =PROPER=, και =NUMBER=.

   Οι εντολές της WikipediaBase και οι τιμές επιστροφής τους
   χρησιμοποιούν κωδικοποίηση σε s-expressions. Η WikipediaBase
   παρέχει τις ακόλουθες λειτουργίες:

*** get

    Δεδομένης μιας class, ενος ονόματος αντικειμένου, και ενός
    τυποποιημένου χαρακτηριστικού, δηλαδή ενός χαρακτηριστικού με
    typecode. Έγκυρα typecodes χαρακτηριστικών είναι =:code= (για ένα
    attribute όνομα όπως στο infobox wiki markup) και =:rendered= (
    για ένα attribute όνομα στο rendered form από το infobox).

**** Types

     Τα Scripts πρέπει να επιστρέφουν μια λίστα από τυποποιημένες
     τιμές, δηλαδή ενα ζευγάρι τιμής - typecode. Έγκυρα typecodes
     είναι:

***** =:HTML=

      Μια συμβολοσειρά προσαρμοσμένη για rendering σαν paragraph level
      HTML. Η συμβολοσειρά πρέπει να είναι escaped για lisp, εννοώντας
      quoted, και με double quotes και backslashes escaped με
      backslashes. Η συμβολοσειρά δεν απαιτείται να περιέχει HTML
      κώδικες. Για παράδειγμα:

      #+BEGIN_SRC lisp
        (get "wikipedia-sea" "Black Sea" (:code "AREA"))
        => ((:html "436,402 km2 (168,500 sq mi)"))

        (get "wikipedia-president" "Bill Clinton" (:code "SUCCESSOR"))
        => ((:html "George W. Bush"))

        (get "wikipedia-president" "Bill Clinton" (:rendered "Succeeded by"))
        => ((:html "George W. Bush"))

      #+END_SRC

***** =:YYYYMMDD=

      Οι αναλυμένες ημερομηνίες αντιπροσωπεύονται σαν αριθμοί,
      χρησιμοποιώντας τον τύπο =YYYYMMDD= με αρνητικούς αριθμούς
      αντιπροσωπεύονται οι πχ ημερομηνίες.

      (Οι μη αναλυμένες ημερομηνίες αντιπροσωπεύονται σαν HTML strings
      χρησιμοποιώντας το =:HTML= typecode.)

      #+BEGIN_SRC lisp
        (get "wikipedia-sea" "Black Sea" (:code "AREA"))
        => ((:html "436,402 km2 (168,500 sq mi)"))

        (get "wikipedia-president" "Bill Clinton" (:code "SUCCESSOR"))
        => ((:html "George W. Bush"))

        (get "wikipedia-president" "Bill Clinton" (:rendered "Succeeded by"))
        => ((:html "George W. Bush"))
      #+END_SRC

***** =:CALCULATED=

      Το Typecode για χαρακτηριστικά υπολογισμένα από την με βάση
      χαρακτηριστικά του άρθρου, πχ., =GENDER= and =NUMBER=. Βλέπε
      παρακάτω στο Special Attributes για την ολοκληρωμένη λίστα των
      υπολογισμένων attributes.

***** =:CODE=

      Ξεπερασμένο συνώνυμο του =:HTML=.

***** =:STRING=

      Ξεπερασμένο συνώνυμο του =:HTML=.

***** Special Attributes

      Μερικά χαρακτηριστικά είναι ειδικά επειδή υπολογίζονται από
      WikipediaBase αντί να είναι τραβηγμένα από infoboxes ή δεν
      παρέχονται άμεσα. Αυτά τα χαρακτηριστικά θα πρέπει να είναι
      ειδικά για =wikibase-term=, =wikibase-person=, και
      =wikipedia-paragraphs=.


****** =SHORT-ARTICLE=, =wikibase-term=

       Η πρώτη παράγραφος του άρθρου, ή αν η πρώτη παράγραφος είναι
       μικρότερη από 350 χαρακτήρες, τότε το μέρος της πρώτης
       παραγράφου έτσι ώστε το άθροισμα των χαρακτήρων να είναι
       τουλάχιστον 350.

       ii. =URL=, =wikibase-term=

       Επιστρέφει το URL του άρθρου ως =((:url URL))=

****** =IMAGE-DATA=, =wikibase-term=

       Επιστρέφει μια λίστα από URLs εικόνων στο περιεχόμενο του
       άρθρου (αποκλείει εικόνες που είναι στη σελίδα αλλά εκτός του
       περιεχομένου του άρθρου). Εάν δεν υπάρχουν εικόνες θα πρέπει να
       επιστρέφει μια κενή λίστα.

       Η "καλύτερη" εικόνα πρέπει να είναι η πρώτη της λίστας, εάν
       υπάρχει εικόνα στην κορυφή του infobox, αυτή θεωρείται η
       καλύτερη εικόνα, διαφορετικά είναι η πρώτη εικόνα που
       εμφανίζεται οπουδήποτε στο άρθρο. Εαν δεν υπάρχει caption, η
       τιμή του caption παραλείπεται

       π.χ., προτιμότερο =((0 "Harimau\_Harimau\_cover.jpg"))=

       από =((0 "Harimau\_Harimau\_cover.jpg" ""))=.


***** =COORDINATES=, =wikibase-term=

      Χαρακτηριστικά που δίνονται στο άρθρο υπολογιζόμενα από το
      γεωγραφικό πλάτος και το μήκος ή, εφόσον κανένα δεν μπορεί να
      βρεθεί, το infobox .  Η τιμή είναι μια λίστα του πλάτους και
      μήκους, πχ. =((:coordinates latitude longitude))=

      #+CAPTION: An example of coordinates in the header
      #+NAME:   fig:coordinate-example
      #+attr_latex: :placement [H] :width \textwidth
      [[./black-sea.png]]


***** =BIRTH-DATE=, =wikibase-person=

      Λαμβάνονται από το infobox ή αν δεν βρεθεί, λαμβάνονται από το άρθρο,

      ή αν δεν βρεθεί, από τις πληροφορίες της κατηγορίας του άρθρου.

      Βασίζεται πάντα στην πρώτη ημερομηνία γέννησης που εντοπίσθηκε και
      ταιριάζει σε μια από τις διάφορες υποστηριζόμενες μορφές. Αν αυτό το
      χαρακτηριστικό έχει μια τιμή,

      τότε το αντικείμενο θεωρείται ότι είναι ένα πρόσωπο με αξία

      στην ιδιότητα ΦΥΛΟ ( βλέπε παρακάτω ) .

      Η τιμή μπορεί να είναι μια a parsed or unparsed date. Parsed dates
      αντιπροσωπεύονται ως αριθμούς , χρησιμοποιώντας τη μορφή YYYYMMDD
      χρησιμοποιώντας αρνητικούς αριθμούς για τις ημερομηνίες Π.Χ.

      Unparsed dates are strings.

***** =DEATH-DATE=, =wikibase-person=

      Λαμβάνονται με παρόμοιο τρόπο όμως το BIRTH-DATE. Επιστρέφει την ίδια
      τιμή όπως

      BIRTH-DATE,εκτός αν το πρόσωπο ζει, τότε βγάζει λάθος τιμή με
      διευκρίνηση

      "Currently alive".

***** =GENDER=, =wikibase-person=

      Υπολογίζεται από το περιεχόμενο της σελίδας βασιζόμενο στα heuristics
      όπως ο αριθμός των ανδρικών ή των θηλυκών αντωνυμιών που
      χρησιμοποιούνται στο κείμενο

***** =NUMBER=, =wikibase-term=

      Το αν η περιγραφόμενη έννοια είναι ενικός ή
      πληθυντικός. Υπολογίζεται από το περιεχόμενο του κειμένου με
      βάση τα χαρακτηριστικά όπως ο αριθμός των φορών που ο τίτλος της
      σελίδας εμφανίζεται στον πληθυντικό. Έχει αξία για όλα τα
      αντικείμενα.

      Επιστρέφει =#t= είναι πληθυντικός, =#f= αν είναι ενικός.

***** =PROPER=, =wikibase-term=

      Το αν η περιγραφόμενη έννοια είναι κύριο όνομα. Υπολογίζεται από
      το περιεχόμενο του κειμένου με βάση τα χαρακτηριστικά όπως ο
      αριθμός των φορών που ο τίτλος της σελίδας εμφανίζεται με
      κεφαλαία γράμματα όταν δεν είναι στην αρχή της σελίδας. Έχει
      τιμή για όλα τα αντικείμενα.

      Επιστρέφει =#t= αν είναι κύριο όνομα, =#f= αν δεν είναι.

**** =get-classes=

     Δεδομένου του ονόματος ενός αντικειμένου , επιστρέφει μια λίστα
     με όλες τις classes οπου ανήκει το αντικείμενο, με τις classes να
     αντιπροσωπεύονται ως lisp-readable strings. Παραδοσιακά τα
     ονόματα των τάξεων δίνονται με μικρά γράμματα χωρίς όμως αυτό να
     είναι απολύτως απαραίτητο.

     #+BEGIN_SRC lisp
       (get-classes "Cardinal (bird)")
       => ("wikibase-term" "wikipedia-paragraphs" "wikipedia-taxobox")

       (get-classes "Hillary Rodham Clinton")
       => ("wikibase-term"
       "wikipedia-paragraphs"
       "wikibase-person"
       "wikipedia-officeholder"
       "wikipedia-person")
     #+END_SRC


**** =get-attributes=

     Δεδομένου του ονόματος μιας τάξης, επιστρέφει έναν κατάλογο με
     όλα τα χαρακτηριστικά της τάξης (δηλαδή όλες οι μεταβλητές που τα
     υλοποιεί infobox), ως lisp-readable strings. Τα ονόματα των
     χαρακτηριστικών δίνονται σε κεφαλαία γράμματα, αλλά αυτό δεν
     αποτελεί απόλυτη απαίτηση.

     #+BEGIN_SRC lisp
       (get-attributes "wikipedia-officeholder" "Barack Obama")
       => ((:CODE "TERM_END3" :VALUE :YYYYMMDD) ...)
     #+END_SRC


**** =Sort-symbols=

     Βάζοντας σε σειρά σύμβολα παίρνει κάθε σύνολο συμβόλων και τα βάζει σε
     σειρά δημιουργώντας υποσύνολα κατα μήκος του σχετικού άρθρου.

     #+BEGIN_SRC lisp
       (sort-symbols  "Obama (surname)" "Barack Obama")
       => (("Barack Obama") ("Obama (surname)"))
     #+END_SRC

**** =sort-symbols-named=

     παίρνει ένα συνώνυμο και ένα σύνολο συμβόλων και τα βάζει σε
     σειρά δημιουργώντας υποσύνολα. Εαν το symbol name είναι το ίδιο
     με το συνώνυμο, το ίδιο και το υποσύνολό του μπαίνουν στην αρχή.

     #+BEGIN_SRC lisp
       (sort-symbols-named
        "cake"
        "Cake (TV series)"
        "Cake (firework)"
        "Cake (film)"
        "Cake (drug)"
        "Cake"
        "Cake (band)"
        "Cake (advertisement)"
        "The Cake")
       => (("Cake")
       ("Cake (band)")
       ("Cake (advertisement)")
       ("Cake (TV series)")
       ("The Cake")
       ("Cake (film)")
       ("Cake (firework)")
       ("Cake (drug)"))
     #+END_SRC

** Getting started

   Η συνολική WikipediaBase βρίσκεται σε ένα git repository στο
   infolab's github orginization page.

   #+BEGIN_SRC sh
     git clone git@github.com:infolab-csail/WikipediaBase
   #+END_SRC

   Το =WikipediaBase= εξαρτάται από πολλά άλλα πακέτα python. Ευτυχώς,
   η python είναι shipped όχι μονο με ένα σπουδαίο package manager
   αλλά επίσης με ένα μηχανισμό που ονομάζεται =virtualenv= το οποίο
   απομονώνει την εγκατάσταση των εξαρτήσεων από το υπόλοιπο σύστημα,
   έτσι αποφεύγονται προβλήματα όπως ασυμβατότητα εκδόσεων.

   ή namespace collisions. Ο τρόπος που αυτό δουλεύει αποτελεσματικά είναι
   με το global

   python installation να είναι το μισό copied και το μισό symlinked σε ένα
   τοπικό directory και τα dependencies να είναι εγκαταστημένα μόνο σε ένα
   τοπικό sandbox.

   Για να δημιουργηθεί και να ενεργοποιηθεί ένα python virtualenv:

   #+BEGIN_SRC sh
     $ virtualenv --no-site-packages py
     $ . py/bin/activate
     $ which python
     /the/local/directory/py/bin/python
   #+END_SRC

   Τώρα που ασφαλώς τα έχουμε εγκαταστήσει όλα θέλουμε χωρίς να
   σπάσουμε κάποιο global installation


   #+BEGIN_SRC sh
     pip install -r requirements.txt
   #+END_SRC

   Θα χρειασθούμε μερικά επιπλέον εργαλεία για να δουλέψει η
   WikipediaBase που θα πρέπει να εγκατασταθούν system wide:

   - Postresql
   - Redis

   Η εγκατάσταση αυτών των πακέτων διαφέρει ανάλογα με το λειτουργικό
   σύστημα ή τον package manager. Και οι δύο είναι βάσεις δεδομένων. Ο
   σκοπός τους είναι η προσωρινή αποθήκευση επαναλαμβανόμενη
   υπολογισμών και για την αποθήκευση ahead-of-time υπολογισμού , όπως
   το όνομα infobox σήμανσης για καθίσταται χάρτες όνομα και συνώνυμα.

** Αρχιτεκτονική

*** Infobox

    Τα Ιnfoboxes είναι πίνακες που χρησιμοποιούνται συνήθως στη
    wikipedia για να παρέχουν μια επισκόπηση των πληροφοριών σε ένα
    άρθρο με ένα ημι δομημένο τρόπο . Infoboxes είναι η κύρια πηγή
    πληροφοριών για τη WikipediaBase


    #+CAPTION: Ένα παράδειγμα ενός infobox
    #+NAME:   fig:infobox-example
    #+attr_latex: :placement [H] :height 12cm
    [[./alonzo-church-infobox.png]]

    Σε ορους mediawiki markup, ένα infobox είναι ένα typed template
    που αποδίδεται σε html, έτσι ώστε οι παρεχόμενες πληροφορίες να
    έχουν νόημα στο πλαίσιο που παρέχονται. Για παράδειγμα:


    #+BEGIN_SRC text
      {{Infobox scientist
      | name              = Gerhard Gentzen
      | image             = Gerhard Gentzen.jpg
      | image_size        =
      | alt               =
      | caption           = Gerhard Gentzen in Prague, 1945.
      | birth_date        = {{Birth date|1909|11|24}}
      | birth_place       = [[Greifswald]], [[Germany]]
      | death_date        = {{Death date and age|1945|8|4|1909|11|24}}
      | death_place       = [[Prague]], [[Czechoslovakia]]
      | nationality       = [[Germany|German]]
      | fields            = [[Mathematics]]
      | workplaces        =
      | alma_mater        = [[University of Gottingen]]
      | doctoral_advisor  = [[Paul Bernays]]
      | doctoral_students =
      | known_for         =
      | awards            =
      }}
    #+END_SRC

    Θα παράξει το εξής¨

    #+CAPTION: Παράδειγμα εξαγωγής infobox
    #+NAME:   fig:redered-infobox-exampl
    #+attr_latex: :placement [H] :height 12cm

    Οι τύποι του Infobox είναι οργανωμένοι με μια αρκετά ευρεία
    ιεραρχία{{{ref(infobox_hierarchy)}}}. Για παράδειγμα
    =Template:Infobox Austrian district= είναι μια ειδική περίπτωση
    ενός =Template:Infobox settlement= και το καθένα είναι rendered
    διαφορετικά. Για το συγκεκριμένο σκοπό, και για να κάνουμε mirror
    το markup ορίζουμε τα infoboxes, ένα infobox \(I\) με
    χαρακτηριστικά \(a_i\) και τιμές \(v_i\) είναι ένα σύνολο από
    ζεύγη \(a_i, v_i\) μαζί με ένα τύπο infobox \(t\). Κάθε
    χαρακτηριστικό \(a_i\) και τιμή \(v_i\) έχουν 2 μορφές:

    - rendered μορφή, \(a^r_i\) και \(v^r_i\) αντίστοιχα, η rendered
      HTML αναπαράσταση
      - Η markup αναπαράσταση, \(a^m_i\) και \(v^m_i\) που είναι η
        mediawiki markup συμβολοσειρά


    Ένα άρθρο μπορεί να έχει περισσότερα από ένα infoboxes, για
    παράδειγμα, το άρθρο για τον Bill Clinton έχει δύο infobox: ένα
    για =Officeholder= και ένα για =Infobox President=. Η class
    =Infobox= είναι ο βασικός τύπος δεδομένων για την πρόσβαση σε
    πληροφορίες από το infobox ενός άρθρου. H =Infobox=, όπως και η
    =Article=, είναι αυτή που θα χρησιμοποιήσει κάποιος όταν
    χρησιμοποιεί τη wikipediabase ως βιβλιοθήκη Python. Οι μέθοδοι που
    παρέχονται από την Infobox δίνουν πρόσβαση στις εξής πληροφορίες:

    - Τυποι ::  επειδή έχουμε ανακτήσει Infobox βασισμένοι σε ένα
         όνομα συμβόλου (π.χ.όνομα της σελίδας ), ένα μοναδικό Infobox
         μπορεί στην πραγματικότητα να είναι μια διεπαφή για πολλαπλά
         infoboxes. Υπάρχει μια ξεχωριστή μέθοδος, που βασίζεται σε
         αυτό, για την ανάκτηση τύπων σε μορφή κατάλληλη για το START.

    - Τιμές αρακτηριστικών :: δεδομένης είτε \(a^r_i\) είτε \(a^m_i\).

    - Ονόματα χαρακτηριστικών :: που παρέχονται με τη χρήση του
         MetaInfobox ( βλέπε παρακάτω )
    - Εξαγωγή των πληροφοριών σε python types :: συγκεκριμένα
         - =dict= για \(a^r_i \rightarrow v^r_i\) or \(a^m_i \rightarrow
           v^m_i\)
         - Το συνολικό infobox rendered, ή σε ένα markup μορφή.

    Τα Infoboxes οργανώνονται σε μια ευρεία ιεραρχία το οποίο στον
    κώδικα του WikiepdiaBase αναφέρεται ως infobox tree. Τo infobox
    tree ανακτάται από τη λίστα της σελίδας wikipedia List of
    infoboxes και χρησιμοποιείται για να συνταχθεί η οντολογία των
    όρων wikipedia.

** MetaInfobox

   Το =MetaInfobox= υπλοποιείται ως μια υποκλάσση του =Infobox= που
   προσδίδει πληροφορία σχετικά με το infobox, εστιάζοντας στη
   αντιστοιχία της rendered μορφής των χαρακτηριστικών με την markup
   μορφή. Έτσι δεδομένου ενός infobox τύπου \(I\) έχει δυνατά
   χαρακτηριστικά \({a_1, ... , a_n}\). Κάθε χαρακτηριστικό έχει δύο
   αναπαραστάσεις:

   - τη markup αναπαράσταση που χρησιμοποιείται στο infobox template.
   - την HTML rendered αναπαράσταση, που είναι το κείμενο που φαίνεται
     στην αριστερή μεριά του πίνακα του infobox στη σελίδα.

   Παραδείγματος χάριν στο =officeholder= infobox υπάρχει ένα
   χαρακτηριστικό με markup αναπαράσταση =predecessor= και μία
   rendered αναπαράσταση =Preceded by=.

   Για να το πετύχει αυτό το =MetaInfobox= χρησιμοποιεί την σελίδα
   τεκμηρίωσης του template για να βρει το markup representation όλων
   των αποδεκτών χαρακτηριστικών ενός τύπου infobox. Στη συνέχεια
   δημιουργεί ένα infobox οπού κάθε χαρακτηριστικό έχει τιμή τη markup
   αναπαράσταση του χαρακτηριστικού αυτού, τυλιγμένη με τη
   συμβολοσειρά =!!!=. (Για παράδειγμα το χαρακτηριστικό με markup
   όνομα =predecessor= θα έχει τιμή =!!!predecessor!!!=). Στη συνέχει
   κανει render το infobox που δημιούργησε και ψάχνει για
   =!!!predecessor!!!= στις rendered τιμές. Θεωρούμε ότι οι τα
   αντίστοιχα rendered ονόματα αντιστοιχούν στα markup
   χαρακτηριστικά. Σημειώστε πως η αντιστοιχεία των rendered
   χαρακτηριστικών με τα markup χαρακτηριστικά δεν είναι αμφοσήμαντη,
   δηλαδή κάθε markup χαρακτηριστικό μπορεί να αντιστοιχεί σε μηδέν η
   περισσότερα rendered χαρακτηριστικά και το αντίστροφο.

   Για παράδειγμα για ένα infobox τύπου =Foo= με αποδεκτά χαρακτηριστικά /A/, /B/,
   /C/ και /D/ το =MetaInfobox= θα δημιουργούσε markup:

   #+BEGIN_EXAMPLE
     {{Infobox Foo
     | A = !!!A!!!
     | B = !!!B!!!
     | C = !!!C!!!
     | D = !!!D!!!
     }}
   #+END_EXAMPLE

   Και η rendered μορφή θα ήταν, ανάλογα με την υλοποίηση του =Foo=
   infobox.

   | Attribute | Value                   |
   |-----------+-------------------------|
   | A         | !!!A!!! !!!B!!! !!!C!!! |
   | B         | !!!A!!! !!!B!!! !!!C!!! |
   | C         | !!!A!!! !!!B!!! !!!C!!! |
   | D         | !!!D!!!                 |

   Έτσι η αντιστοιχία γίνεται σχετικά εμφανής.

** Article

   Η class =Article= είναι υπεύθυνη για την πρόσβαση σε κάθε πόρο
   σχετικό με το άρθρο γενικότερα. Αυτό περιλαμβάνει τις παραγράφους,
   επικεφαλίδες, τον πηγαίο markup κωδικα και τις κατηγορίες
   MediaWiki.

** Fetcher

   Η κλάση =Fetcher= είναι μια αφαίρεση από την επικοινωνία της
   WikipediaBase με τον έξω κόσμο. Είναι ένα μονήρες αντικείμενο που
   υλοποιεί μια συγκεκριμένη διεπαφή.

   Τα υλοποιημένα =Fetchers= σε κληρονομική ιεραρχία που φαίνεται από
   την παρακάτω λίστα.

   - =BaseFetcher= :: είναι η υπερκλάση όλων των fetchers. Θα
        επιστρέψει το ιδιο σύμβολο αντί να προσπαθήσει να το επιλύσει
        με οποιονδήποτε τρόπο. Κάνουμε override αυτή τη λειτουργία
        στις κληρονόμους κλάσεις για να υλοποιήσουμε τη λογική της
        διεπαφής με τον έξω κόσμο
   - =Fetcher= :: Υλοποιεί τη βασική λειτουργία. Αναζητά πληροφορίες
        απο το wikipedia.org. Είναι δυνατόν να κατευθύνουμε ένα
        fetcher αυτό προς ένα mirror αλλά η εκτέλεση σε
        wikipedia-mirror είναι από άποψη πόρων εκτέλεσης απαγορευτική.
   - =CachingFetcher= :: κληρονομεί fetcher και διατηρεί τη
        λειτουργικότητα , μόνο που χρησιμοποιεί Redis για την
        προσωρινή αποθήκευση των fetched συμβόλων .  Είναι η
        προεπιλεγμένη fetcher class.
   - =StaticFetcher= :: είναι μια κλάση που υλοποιεί το interface
        BaseFetcher αλλά αντί να φτάσει σε κάποια πηγή δεδομένων για
        τα δεδομένα η τιμές επιστροφής είναι στατικά
        ορισμένες. Χρησιμοποιείται κυρίως από το =MetaInfobox= για να
        χρησιμοποιεί τη λειτουργία του Infobox να μεταφέρει αυθαίρετες
        πληροφορίες.

   Από προεπιλογή, το markup προέρχεται μια βάση δεδομένων. Αν η
   παράμετρος =force_live= έχει οριστεί σε =True= τότε το markup θα
   ληφθεί από live wikipedia.org.  Όταν οι δοκιμές τρέχουν στο
   TravisCI{{{ref(travis)}}}, θέλουμε πάντα να χρησιμοποιούνται
   ζωντανά δεδομένα. Ελέγχουμε αν το Travis εκτελεί δοκιμές
   κοιτάζοντας τη μεταβλητή =WIKIPEDIABASE_FORCE_LIVE= μεταβλητή
   περιβάλλοντος.

** Renderer

   =Renderers= είναι μονήρεις classes, χρήσιμες για την απόδοση
   MediaWiki markup σε HTML . Αρχικά το wikiepedia sandbox
   χρησιμοποιήθηκε από τη wikipediabase για την απόδοση σελίδων,
   επειδή είναι ελαφρώς ταχύτερο από την API, αλλά το wikipedia-mirror
   ήταν πολύ αργό και το wikipedia.org το θεωρούσε κατάχρηση της
   υπηρεσίας και μπλόκαρε το IP μας μετά από μερικά τεστ. Γι' αυτό το
   λόγο τελικά μεταπηδήσαμε στο API με Redis caching, το οποίο
   λειτουργούν αρκετά καλά, μιας που τα =Renderer= αντικείμενα
   καταλήγουν να χρησιμοποιούνται μόνο απο το MetaInfobox, το οποίο
   έχει ένα αρκετά περιορισμένο πεδίο εφαρμογής , και έτσι το cache να
   χάνει σπάνια.

   Μια ενδιαφέρουσα πληροφορία για την κατηγορία =Renderer= ήταν ότι
   αυτός ήταν ο λόγος που ένα ζευγάρι CSAIL IPs να αποκλειστεί
   προσωρινά από την επεξεργασία της wikipedia. Ενώ η wikipedia.org
   έχει μια πολύ επιεική πολιτική όταν πρόκειται για την αποκλεισμό
   των ανθρώπων που έχουν κανει spamming τους servers,
   επαναλαμβανόμενες δοκιμές της κατηγορίας Renderer με στόχευση
   wikipedia sandbox προκάλεσε το ip του δοκιμαστικού μηχανήμτος να
   αποκλεισθεί προσωρινά με το σκεπτικό ότι "η δραστηριότητα του δεν
   προάγει την βελτίωση της wikipedia". Εμείς επανατοποθετήσαμε το
   =Renderer= να χρησιμοποιηεί το wikipedia API και ποτέ δεν είχαμε
   ξανά πρόβλημα με την ρύθμιση της wikipedia.

** Pipeline

   Κατά την επίλυση ενός ερωτήματος η WikipediaBase ενεργοποιεί ένα
   pipeline ενοτήτων για να διαπιστωθεί ποιος είναι ο καλύτερος τρόπος
   απάντησης.

*** Frontend

    Η WikipediaBase μπορεί να χρησιμοποιηθεί ως βιβλιοθήκη αλλά ο
    πρωταρχικός της λειτουργία είναι ως backend στο START. Η
    επικοινωνία μεταξύ START και WikipediaBase γίνεται πάνω από ένα
    plaintext telnet σύνδεσής στην πόρτα 8023 χρησιμοποιώντας
    sexpressions. Το frontend χειρίζεται το δίκτυο σύνδεσης με το
    START, μεταφράζει τις προσλαμβανόμενες ερωτήσεις σε κλήσεις της
    Knowledgebase και στη συνέχεια μεταφράζει την αντίδραση της
    Knowledgebase σε κατάλληλα διαμορφωμένες εκφράσεις και τις
    επιστρέφει πίσω στο telnet connection.

*** Knowledgebase

    Η knowledgebase είναι το σημείο εισαγωγής στο rest της wikipediabase.

    Χρησιμοποιεί μοτίβο Provider/Acquirer να παρέχει διαφανή διεπαφή
    της frontend με αυθαίρετες μεθόδους. Οι μέθοδοι αυτοί είναι
    υπεύθυνοι για την επιλογή του αν θέλουμε να καταλήξουμε σε
    classifiers, resolvers οποιοδήποτε άλλο μηχανισμό για να δοθεί
    απάντηση στο ερώτημα. Διαθέσιμοι classifiers και resolvers
    γίνονται προσβάσιμοι αυτόματα στη knowledgebase χρησιμοποιώντας τη
    βασική τους κλάση.

    # MARK
*** Classifiers

    Κάθε Classifier είναι μονήρης κλάση και υλοποιεί μια ευρετική για
    για να συνάξει μια λίστα από κατηγορίες ενός αντικειμένου. Ένα
    αντικείμενο μπορεί να επιστρέφει μηδέν ή περισσότερες
    κατηγορίες.

    Συνήθως, ένας Classifier θα συμπεράνει μόνο αν ένα αντικείμενο
    πράγματι αναστέλλει μια συγκεκριμένη κατηγορία ή όχι, αλλά αυτό
    δεν είναι απαραίτητο.

**** Term

     Ο =TermClassifier= απλά αναθέτει την κατηγορία
     =wikipedia-term=. Η Wikipediabase διαπραγματεύεται μόνο μόνο με
     πληροφορίες σχετικές με τη wikipedia. Συνεπώς όλες οι έννοιες που
     συναντώνται είναι σε αυτήν την κατηγορία.

**** Infobox

     Το =InfoboxClassifier= αναθέτει σε ένα όρο την κατηγορία
     infobox. Για παράδειγμα η σελίδα Bill Clinton περιέχει το
     infobox:


     #+BEGIN_EXAMPLE
       {{Infobox president
       |name          = Bill Clinton
       |image         = 44 Bill Clinton 3x4.jpg{{!}}border
       [...]
       }}
     #+END_EXAMPLE

     Και γι αυτό λαμβάνει την κατηγορία =wikipedia-president=.

**** Person

     Το =PersonClassifier= αναθέτει την κατηγορία =wikibase-person=
     χρησιμοποιώντας κάποια χαρκτηριστικά με την σειρά που
     περιγράφονται:

***** Category regexes

      Χρησιμοποιεί τις ακόλουθες συνήθεις εκφράσεις (regular
      expressions) για να ταυτίσει τις κατηγορίες ενός άρθρου.

      - =.* person=
      - =\^\d+ deaths.*=
      - =\^\d+ births.*=
      - =.* actors=
      - =.* deities=
      - =.* gods=
      - =.* goddesses=
      - =.* musicians=
      - =.* players=
      - =.* singers=

***** Category regex excludes

      Αποκλείει τις ακόλουθες regexes.

      - =\sbased on\s=
      - =\sabout\s=
      - =lists of\s=
      - =animal\s=

***** Category matches

      Γνωρίζουμε ότι ένα άρθρο αναφέρεται σε ένα πρόσωπο εάν η σελίδα ανήκει
      σε μια από τις ακόλουθες mediawikia κατηγορίες:

      - american actors
      - american television actor stubs
      - american television actors
      - architects
      - british mps
      - character actors
      - computer scientist
      - dead people rumoured to be living
      - deities
      - disappeared people
      - fictional characters
      - film actors
      - living people
      - musician stubs
      - singer stubs
      - star stubs
      - united kingdom writer stubs
      - united states singer stubs
      - writer stubs
      - year of birth missing
      - year of death missing

      Για ένα παράδειγμα δείτε το παράρτημα.

      Όπως είναι φανερό η λίστα με τις κατηγορίες είναι αθυαίρετη και
      όχι πλήρης. Πολλαπλές μέθοδοι μπορούν να χρησιμοποιηθούν για να
      διορθωθεί αυτό.

      Μερικές από αυτές είναι:

      - Μέθοδοι με Supervised machine learning όπως SVM χρησιμοποιώντας άλλες
        μεθόδους να ορίσουν ένα πρόσωπο και να δημιουργήσουν εκπαιδευτικές
        ομάδες.
      - Εμπλουτίζοντας την υπάρχουσα λίστα κατηγοριών χρησιμοποιώντας
        στατιστικά από κατηγορίες άρθρων που έχουμε βρει με άλλους
        τρόπους ότι αναφέρονται σε πρόσωπα.

*** Resolvers

    Οι =Resolvers= είναι επίσης μονήρεις κλάσεις αλλά ο σκοπός τους
    είναι να βρούνε την τιμή του αναζητούμενου χαρακτηριστικού. Όλοι
    οι resolvers κληρονομούν από την κλάση =BaseResolver= και πρέπει
    να υλοποιούν τις ακόλουθες μεθόδους:

    - =resolve(class, symbol, attribute)= που δίνει την τιμή ενός
      χαρακτηριστικού δεδομένου του συμβόλου και της κλάσης.
    - =attributes(class, symbol)=: που δίνει μια λίστα από τα
      χαρακτηριστικά που μπορεί να επιλύσει ο συγκεκριμένος resolver
      για το συγκεκριμένο άρθρο δεδομένης της class του.

    Οι υλοποιημένοι resolvers είναι οι ακόλουθοι:

    - Error :: ο ελάχιστης προτερεότητας resolver. Επιλύεται πάντα σε
         σφάλμα.
    - Infobox :: Επιλύει χαρακτηριστικά που αναφέρονται σε κάποιο
         πεδίο του infobox
    - Person :: επιλύει τα ακόλουθα ειδικά χαρακτηριστικά των άρθρων
         που αναφέρονται σε πρόσωπα
         - =birth-date=
         - =death-date=
         - =gender=
    - Sections :: το περιεχόμενοτων κεφαλαίων σε ένα άρθρο.
    - Term :: επιλύει ένα συγκεκριμένο σύνολο χαρακτηριστικών,
         - =coordinates= /Οι συντεταγμένες μιας γεωγραφικής περιοχής/
         - =image= /Την εικόνα μέσα στο infobox./
         - =number= /Αληθής τιμή αν το σύμβολο είναι στον πληθυντικό (πχ
           The Beatles)/
         - =proper= /Αληθής αν αναφέρεται σε κύριο όνομα./
         - =short-article= /Περίληψη του άρθρου, τυπικά η πρώτη παράγραφος./
         - =url= /Η διεύθυνση του άρθρου./
         - =word-cout= /Το μέγεθος του άρθρου σε λέξεις./

*** Lisp types

    Ο τύπος Lisp είναι περιτυλίγματα (wrappers) για python αντικείμενα
    ή τιμές που παρουσιάζονται σε μορφή s-expression που το START
    μπορεί να κατανοήσει. Έχουν δημιουργηθεί είτε από το ανεπεξέργαστο
    ερώτημα και έχουν ξετυλιγεί (unwrapped) ώστε να είναι χρήσιμα στον
    αγωγό (pipeline), ή από την απάντηση που δίνει η WikipediaBase και
    στη συνέχεια κωδικοποιούνται σε ένα string και αποστέλλονται μέσω
    telnet στο START.


** Το μοντέλο provider/acqirer

   Η WikipediaBase προσπαθεί να είναι modular και με δυνατότητα
   επέκτασης.  Για να επιτευχθεί αυτό, Συχνά είναι χρήσιμο να
   συμπλέκει πολλαπλές πηγές του ίδιου τύπου του πόρου δεδομένων. Αυτό
   είναι ιδιαίτερα χρήσιμο κατά την πρόσβαση ευρετικών μεθόδων όπως
   των classifiers που είδαμε παραπάνω. Για την προώθηση του
   modularity και για να αποφευχθεί ισχυρή αλληλεξάρτηση των
   υποσυστημάτων δημιουργήθηκε το μοντέλο provider/acquirer.

   Ο Provider είναι ένα αντικείμενο μέσω του οποίου μπορούμε να
   διαχειριστούμε πηγές που είναι αποθηκευμένες ως ζεύγη κλειδιού -
   τιμής. Η κλάση Provider προσφέρει decorators για να κάνει αυτή της
   διάταξη εύκολη για τον προγραμματιστή. Ένας Acquirer έχει διαφανή
   (transparent) πρόσβαση στους πόρους πολλαπλών =Providers= σαν να
   ήταν ένα ενιαίο σύνολο κλειδιών. Αυτό το πρότυπο κυρίως
   χρησιμοποιείται για την =KnowledgeBase= ώστε να παρέχει το Frontend
   με τρόπο πρόσβασης στις πηγές.

*** COMMENT Παράδειγμα

    Εκθέτουμε το μοτίβο με ένα παράδειγμα ενθέτοντας μια μικρή lisp
    μέσα στην python και χειριζόμενοι το state του εκτελούμενου
    προγράμματος με providers και acquirers.

    #+BEGIN_SRC python
      from wikipediabase.provider import Provider, Acquirer, provide


      class EvalContext(Acquirer):
          def __init__(self, closures):
              super(EvalContext, self).__init__(closures)
              self.closures = closures

          def __call__(self, _ctx, expr):
              if isinstance(expr, list):
                  # Handle quotes
                  if expr[0] is 'quote':
                      return expr[1]

                  # Call the lambda
                  fn = self(_ctx, expr[0])
                  return fn(self, *[self(_ctx, e) for e in expr[1:]])

              if isinstance(expr, basestring) and expr in self.resources():
                  return self(_ctx, self.resources()[expr])

              return expr


      class Lambda(Acquirer):
          def __init__(self, args, expr, env):
              # Get your symbols from all the available closures plus an
              # extra for local variables
              super(Lambda, self).__init__([env] + [Symbols()])
              self.args = args
              self.expr = expr

          def __call__(self, _ctx, *args):
              # Add another closure to the list
              arg_provider = Provider();
              for s, v in zip(self.args, args):
                  arg_provider.provide(s, v)

              # Build an eval context and run it
              ctx = EvalContext([arg_provider, Provider(self.resources())])
              return [ctx(ctx, e) for e in self.expr][-1]

      class Symbols(Provider):
          @provide('setq')
          def setq(self, ctx, symbol, val):
              self.provide(symbol, val)

      class Builtins(Provider):
          @provide('lambda')
          def _lambda(self, ctx, args, *body):
              return Lambda(args, list(body), Provider(ctx.resources()))

          @provide('if')
          def _if(self, ctx, proposition, then, _else):
              if ctx(ctx, proposition):
                  return ctx(ctx, then)
              else:
                  return ctx(ctx, _else)

      GLOBAL_EVAL = EvalContext([Builtins(), Symbols()])
    #+END_SRC

    Αυτή η μικρή lisp αν και πρωτόγονη υποστηρίζει:

    - lambdas
    - A global symbol table
    - lexical scoping
    - conditionals
    - Quoted literals

    Είναι πολύ μακριά από το μια χρησιμοποιημένη γλώσσα αλλά μπορεί να
    πετύχει μερικά ενδιαφέροντα κόλπα:

    Μπορούμε να χρησιμοποιήσουμε python types:

    #+BEGIN_SRC python
      >>> GLOBAL_EVAL({}, 1)
      1
      >>> GLOBAL_EVAL({}, True)
      True
      >>> GLOBAL_EVAL({}, "hello")
      'hello'
      >>> GLOBAL_EVAL({}, list)
      <type 'list'>
    #+END_SRC


    Μπορούμε να ορίσουμε lambdas και να τις καλέσουμ. Το ακόλουθο
    είναι ισοδύναμο με το \((\lambda a. a) 1\), το οποίο πρέπει να
    εκτιμηθεί στην τιμή =1=:

    #+BEGIN_SRC python
      >>> GLOBAL_EVAL({}, [["lambda", ['quote', ['a']], 'a'], 1])
      1
    #+END_SRC

    Η μικρή μας lisp δεν είναι pure εφ όσον έχουμε mutable global
    symbol table. Αυτό σημαίνει πως η σειρά των διεργασιών έχει
    σημασία. Εφ όσον δεν έχουμε =progn= η άλλα macros συνηθισμένα σε
    lisp dialects ο καλύτερος τρόπος να κάνουμε διεργασίες σε σειρά
    είναι να τις εντάξουμε σε ένα lambda και να το εκτιμήσουμε
    (evaluate).

    #+BEGIN_SRC python
      >>> GLOBAL_EVAL({}, [['lambda', ['quote', []], ['setq', 'b', 2], 'b']])
      2
    #+END_SRC

    Ο προσεκτικός αναγνώστης ίσως παρατηρήσει ότι η λίστα για τα
    lambda arguments είναι quoted. Ο λόγος γι αυτό είναι ότι δεν
    θέλουμε η λίστα να εκτιμηθεί.

    Πίσω στην αρχική μας εργασία. Σε κάθε σημείο του κώδικα το κάθε
    σύμβολο λαμβάνει τιμές από πολλαπλές πηγές. Με σειρά προτεραιότητας:

    - The local closure
    - The arguments of the lambda
    - Builtin functions

    Όλα τα προηγούμενα περιληπτικοποιούνται χρησιμοποιώντας το
    provider-aquirer model.

    Σε κάθε σημείο ένα διαφορετικό =EvaluationContext= είναι υπεύθυνο
    για την εκτίμηση και κάθε =EvaluationContext= έχει πρόσβαση στα
    γνωστά σύμβολα του μέσω μιας array of providers τα οποία
    περιληπτικοποιούνται χρησιμοποιώντας το υπό συζήτηση μοντέλο.

** Testing

   Η καλή λειτουργία της WikipediaBase εξασφαλίζεται από μια
   ολοκληρωμένη σειρά δοκιμών των unit tests, functional tests και
   regression tests. Τα Unit tests ελέγχουν μια μικρή ομάδα του
   functionality, το οποίο έχει συντεθεί για την δημιουργία του όλου
   συστήματος. Για το unit testing χρησιμοποιούμε την default
   βιβλιοθήκη python για testing. Κάθε τεστ είναι μια κλάση μου
   κληρονομεί από την κλάση =TestCase= και υλοποιεί το interface της
   που περιγράφεται παρακάτω.

   Τα Functional tests είναι γραμμένα από πριν, κατά τη διάρκεια ή
   λίγο μετά της δημιουργίας του συστήματος και διεκδικούν τη σωστή
   συνολική λειτουργία του συστήματος. Τα Regression tests είναι πολύ
   παρόμοια με τα to functional tests . Αποδεικνύουν ότι όταν βρεθεί
   ένα σφάλμα(bug)το διορθώνουν και επιβεβαιώνουν ότι δεν θα
   εμφανισθεί ξανά αργότερα. Τα Functional και τα regression tests
   είναι τοποθετημένα στα tests/examples.py

   Σχεδόν όλα τα τεστ ξεκινούν με τον ακόλουθο κώδικα:


   #+BEGIN_SRC python
     from __future__ import unicode_literals

     try:
         import unittest2 as unittest
     except ImportError:
         import unittest

     from wikipediabase import fetcher
   #+END_SRC


   Το παραπάνω είναι ειδικό για το the fetcher module. Όπως είναι
   προφανές χρησιμοποιούμε το unittest module από την βιβλιοθήκη
   python. Το test το ίδιο έχει το ακόλουθο format:

   #+BEGIN_SRC python
     class TestFetcher(unittest.TestCase):

         def setUp(self):
             self.fetcher = fetcher.get_fetcher()

         def test_html(self):
             html = self.fetcher.html_source("Led Zeppelin")
             self.assertIn("Jimmy Page", html)
   #+END_SRC

   Η setUp μέθοδος runs πρίν από κάθε τεστ του νTestCase. Τα τεστ του
   testcase αντιπροσωπαε΄υονται από μεθόδους της class το οπίων το
   όνομα αρχίζει με =test\_=. Στην συγκεκριμένη περίπτωση παίρνουμε
   την σελίδα της wikipedia για το /Led Zeppelin/ ότι το όνομα /Jimmy
   Page/ αναφέρται τουλάχιστον μια φορά. Αυτό φανερά δεν συνάδει ότι
   το fetcher δεν φέρνει για παράδειγμα την σελίδα για το /Yardbirds,
   Page's first band/. Γαι αυτό το λόγο γράφουμε μερικά από αυτού του
   είδους τεστ.

   Στην περίπτωση του fetcher, για να ακολουθήσουμε το παράδειγμα , το
   συνολικό τεστ υπάρχει στο παράρτημα.

   Εφαρμόσαμε το εργαλείο nosetests να βρούμε και να τρέξουμε τα
   τεστ. Για να το κάνουμε αυτό το προσθέσαμε σαν προαπαιτούμενο στο
   /setup.py/.

   #+BEGIN_SRC python
     from setuptools import setup

     setup(
         tests_require=[
             'nose>=1.0',
             ...
         ],
         ...
         test_suite='nose.collector',
         ...
     )
   #+END_SRC


   Στη συνέχει να τρέξουμε τα τεστ:

   #+BEGIN_SRC sh
     $ python setup.py test
   #+END_SRC

   Η Nose θα βρει όκα τα αρχεία τα οποία είναι στα tests/ και έχουν τ
   πρόθεμα =test\_=, για παράδειγμα =test\_fetcher.py=. Μέσα σ αυτά τα
   αρχεία η nose θα αναζητήσει subclass της =TestCase= και των οποίων
   το όνομα αρχίζει με =Test=, για παράδειγμα =TestFetcher=. Στη
   συνέχεια τρέχει όλες τις μεθόδους από τις collected classes που
   έχουν το προθεμ =test\_=. Είναι επίσης δυνατό να τρέξει
   συγκεκριμένα τεστ τεστ.

   #+BEGIN_SRC sh
     $ python setup.py test --help
     Common commands: (see '--help-commands' for more)

       setup.py build      will build the package underneath 'build/'
       setup.py install    will install the package

     Global options:
       --verbose (-v)  run verbosely (default)
       --quiet (-q)    run quietly (turns verbosity off)
       --dry-run (-n)  don't actually do anything
       --help (-h)     show detailed help message
       --no-user-cfg   ignore pydistutils.cfg in your home directory

     Options for 'test' command:
       --test-module (-m)  Run 'test_suite' in specified module
       --test-suite (-s)   Test suite to run (e.g. 'some_module.test_suite')
       --test-runner (-r)  Test runner to use

     usage: setup.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]
        or: setup.py --help [cmd1 cmd2 ...]
        or: setup.py --help-commands
        or: setup.py cmd --help
   #+END_SRC

   Δείτε το παράρτημα για επιτυχημένη εκτέλεση των τεστ.

** Συνώνυμα

   Πριν μιλήσουμε για τα συνώνυμα είναι σημαντικό να ορίσουμε τα
   σύμβολα στο πεδίο του omnibase universe:

   Σύμβολα είναι ταυτοποιητές των "αντικειμένων" "objects" στις πήγες
   των πληροφοριών (ο όρος "σύμβολο"("symbol") είναι ατυχής γιατί έχει
   διάφορες έννοιες στην επιστήμη των υπολογιστών. Δυστυχώς έχει
   μείνει για ιστορικούς λόγους.)

   Δεδομένου ότι η γλώσσα τείνει να έχουν πολλαπλές λέξεις που
   αναφέρονται στο ίδιο πράγμα, Είναι επιτακτική η ανάγκη να
   καθορισθούν ονόματα για τα σύμβολα. Συνώνυμα είναι τα ονόματα τα
   οποία οι χρήστες μπορούν να χρησιμοποιήσουν για να αναφερθούν στα
   σύμβολα.

   (Ο όρος συνώνυμα "synonym" είναι ατυχής γιατί είναι one-way mapping
   -"gloss" θα ήταν καλύτερος όρος αλλά έμεινε ο όρος συνώνυμα για
   ιστορικούς λόγους)

   Ο ορισμός συνωνύμων είναι δουλειά του backend. Για το λόγο αυτό
   αναλαμβάνει η WikipediaBase να ορίσει τα απαιτούμενα συνώνυμα.


*** Καλά και κακά συνώνυμα

    Υπάρχουν κανόνες για το ποιο είναι καλό ή κακό συνώνυμο

    - Δεν πρέπει να ξεκινούν με άρθρα ("the", "a", "an")
    - Δεν πρέπει να ξεκινούν με "File:" or "TimedText:".
    - Δεν πρέπει να περιέχουν HTML anchors. Πχ
      "Alexander_Pushkin#Legacy"
    - Δεν πρέπει να ξεκινούν με τα ακόλουθα:
      - "List of "
      - "Lists of "
      - "Wikipedia: "
      - "Category: "
      - ":Category: "
      - "User: "
      - "Image: "
      - "Media: "
      - "Arbitration in location"
      - "Communications in location"
      - "Constitutional history of location"
      - "Economy of location"
      - "Demographics of location"
      - "Foreign relations of location"
      - "Geography of location"
      - "History of location"
      - "Military of location"
      - "Politics of location"
      - "Transport in location"
      - "Outline of topic"
    - Δεν πρέπει να ταιριάζει =\d\d\d\d in location= ή =location in
      \d\d\d\d=
    - Δεν πρέπει να είναι ονόματα των disabiguation pages. Για να το
      κάνουμε αυτό έτσι ώστε να συμπεριλαμβάνει όλες τις σχετικές
      σελίδες, συμπεριλαμβανομένων των τυπογραφικών λαθών, αυτό
      σημαίνει σύμβολα που ταιριάζουν με =\([Dd]isambig[\^)]*\)=

    - Συνώνυμα που ότι τόσο α) θα μπορούσαν να εκληφθούν ότι ξεκινούν
      με άρθρα και β ) μπορεί να υποτάσσουν κάτι χρήσιμο . Αυτό
      σημαίνει ότι για παράδειγμα « A. House» ( συνώνυμο του «Abraham
      House") είναι ελλιπών προδιαγραφών διότι ενδέχεται να
      παραπλανήσει START στην περίπτωση των ερωτήσεων όπως «Πόσο
      κοστίζει ένα σπίτι στη Silicon Valley;» . Αφετέρου "a priori "
      μπορεί να διατηρηθεί επειδή δεν υπάρχουν λογικές ερωτήματα όπου
      "α" είναι ένα άρθρο πριν "priori" .

*** Παραγωγή συνωνύμων

    Για να συμβιβάσουμε αυτούς τους περιορισμούς δύο μέθοδοι
    χρησιμοποιούνται qualification και modification των υποψήφιων
    συνονύμων. Πρώτα προσπαθούμε τη modification και αν αυτό αποτύχει
    επιχειρούμε να κάνουμε disqualify. Οι κανόνες για modification
    έχουν ως εξής:

    - Να διαγράψουμε τα άρθρα από την αρχή ενός συνωνύμου:
      - "A "
      - "An "
      - "The "
      - "(The) "
      - The&nbsp;
      - κτλ
    - Να δημιουργούμε και και τα δύο versions, με και χωρίς
      παρενθέσεις. Πχ, δεδομένου του συμβόλου "Raven (journal)"
      δημιουργούμε:

      - "Raven (journal)"
      - "Raven"

    - Χρησιμοποιούμε τη συμβολοσειρά πριν και μετά τοslash, αλλά όχι
      το αρχικό symbol, πχ. δεδομένου του συμβόλου "Russian
      language/Russian alphabet" δημιουργούμε

      - "Russian language"
      - "Russian alphabet"

    - Ανάστροφη των ανεστραμμένων συμβόλων με κόμματα. Πχ δεδομένου
      "Congo, Democratic Republic Of The", αναστρέφουμε για να πάρουμε
      "Democratic Republic Of The Congo"
    - Ώς συνήθως , απορρίπτουμε leading articles εάν είναι
      αναγκαίο. Π.χ. δοθέντος συμβόλου "Golden ratio, the" το
      αντικαθιστούμε με "the Golden ratio", στη συνέχεια διαγράφουμε
      τα άρα για πάρουμε: "Golden ratio" το ίδιο συμβάνει για τα a,
      an, κτλ.

    Με αυτό τον τρόπο κάναμε generate ένα αρχικό πακέτο συνωνύμων από
    το ίδιο το όνομα του αντικειμένου. Επιπλέον μπορούμε να κάνουμε
    generate ένα πακέτο από από τα wikipedia redirects στο άρθρο.  Η
    Wikipedia παρέχει ένα SQL dump για όλα τα redirects. Για να
    φορτώσουμε τον πίνακα στην βάση δεδομένων όπου έχουμε φορτώσει τα
    δεδομένε α της wikipedia, πρέπει να φορτώσουμε τον πίνακα των
    redirects:

    #+BEGIN_SRC sh
     wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-redirect.sql.gz \
       -O redirect.sql.gz && gzcat redirect.sql.gz | mysql
    #+END_SRC

    Και στη συνέχεια το SQL db για να βρούμε όλα τα συνώνυμα του (καλά
    και κακά) Bill Clinton μπορούμε να:

    #+BEGIN_SRC sql
     select page_title, rd_title from redirect join page on rd_from = page_id and (rd_title = "Bill_Clinton" or page_title = "Bill_Clinton");
    #+END_SRC

    Για το πλήρες output δείτε στο παράρτημα.

** Databases and data sources

*** HTML and MediaWiki API

    Η αρχική προσέγγιση για να πάρουμε τα δεδομένα είναι να ανασύρουμε
    τις φυσιολογικές HTML εκδόσεις των άρθρων της wikipedia και
    χρησιμοποιώντας edit pages να ανασύρουμε το mediawiki
    markup. Ανεξαιρέτως χρησιμοποιήσαμε το αρχικό wikipedia.org site
    για λόγους performance (Βλέπε κεφάλαιο wikipedia-mirror runtime
    performance).

    Το Mediawiki παρέχει a RESTful API για όλη την απαιτούμενη
    λειτουργία (functionality). Η βαική αρχή είναι ότι κάποιος μπορεί
    να στείλει αιτήματα με μεθόδους POST ή GET και να λαμβάνει
    απάντηση με την μορφή XML ή JSON. Η προτιμητέα απάντηση για την
    WikipediaBase ήταν ν α στέλνονται GET HTTP αιτήματα και να
    λαμβάνουν JSON δεδομένα. Το GET επιλέχθηκε ειδικά προτάθηκε στην
    mediawiki API page γιατί caching συμβαίνει στο HTTP
    επίπεδο. Σύμφωνα με τις οδηγίες του HTTP τα POST αιτήματα δεν
    μπορούν να cached. Για το λόγο αυτό όταν διαβάζει κάποιος δεδομένα
    από web service API, θα πρέπει να χρησιμοποιεί GET αιτήματα και
    όχι POST.

    Επίσης πρέπει να σημειωθεί ότι ένα αίτημα δεν μπορεί να εκτελεσθεί
    από cache εκτός αν το URL είναι ακριβώς το ίδιο. Εάν ζητήσεις ένα
    αίτημα για =api.php?titles=Foo|Bar|Hello=, και αποθηκεύσει το
    αποτέλεσμα, μετά =api.php?titles=Hello|Bar|Hello|Foo= δεν θα βρει
    την απάντηση στην cache παρ όλο που είναι το ίδιο αιτήματα!

    Η αναπαράσταση JSON επιλέχθηκε άπλα επειδή η βιβλιοθήκη json της
    python πολύ πιο εύκολη στη χρήση από την lxml, τη βιβλιοθήκη που
    χρησιμοποιούμε για XML/HTML parsing.

*** Caching

    Η Wikipediabase χρησιμοποιεί κυρίως έναν απομακρυσμένο χώρο
    αποθήκευσης δεδομένων και εφαρμόζει το mediawiki interface (δηλαδή
    το mediawiki). Προσπαθεί να αντιμετωπίσει ζητήματα επιδόσεων που
    προκύπτουν με την προσωρινή αποθήκευση των σελίδων σε μια τοπική
    key-value βάση δεδομένων. Το interface με τη βάση δεδομένων
    αφαιρείται με τη χρήση ενός python dictionary-style interface, το
    οποίο εφαρμόζεται στο =persistentkv.py=. Εφαρμογές των backends
    παρουσιάζονται παρακάτω, αλλά είναι ασήμαντο να παρέχεται κάθε
    backend που ο καθένας συναντά. Ένα άλλο χαρακτηριστικό που το
    interface στην βάση δεδομένων πρέπει να μπορεί να χρησιμοποιήσει
    είναι η κωδικοποίηση των αποθηκευμένων αντικειμένων. Επειδή όλη η
    αποθηκευμένη πηροφορία είναι κείμενο, η βάση δεδομένων πρέπει ν
    είναι ικανή να ανασύρει ακριβώς το κείμενο που έχει αποθηκευθεί
    λαμβάνοντας υπόψη την κωδικοποίηση.  Λόγω των περιορισμών του
    DBM’s τα κλειδιά (keys) πρέπει να είναι μόνο κωδικοποιημένα
    ASCII. H βασική κλάση για αλληλεπίδραση με την βάση δεδομένων, το
    =EncodedDict=, εφρμόζει τις μεθόδους =_encode_key= και
    =_decode_key= για να παραχωρήσει ένα εύκολο hook εφαρμογών με
    σκοπό να διαχειρισθεί πιθανές καταστάσεις.

**** DBM

     Διάφορες υλοποιήσεις dbm{{{ref(dbm)}}} παρέχονται από την σταθερή βιβλιοθήκη της
     python Όμως καμιά από τις standard βιβλιοθήκες της python δεν
     είναι μέρος της σταθερής βιβλιοθήκης της python. Μερικές εφαρμογές
     DBM που είναι διαθέσιμες μέσω της σταθερής βιβλιοθήκης της python
     είναι:

     - AnyDBM
     - GNU DBM
     - Berkeley DBM

     Είναι σημαντικό να αναφέρουμε ότι η ομαλή λειτουργία αυτών των
     βιβλιοθηκών εξαρτάται σε σημαντικό βαθμό από την βασική πλατφόρμα
     όπως το λειτουργικό.

     Όπως αναφέρθηκε παραπάνω οι interface classes του DBM μεταφράζουν
     από και προς ASCII. Ο ακριβής μηχανισμός που γίνεται αυτό είναι:

     # Σοθρψε

**** SQLite

     Η SQLite{{{ref(sqlite)}}} επίσης χρησιμοποιείται ως caching
     backend βάση δεδομένων. Δυστυχώς η αποτελεσματικότητά του στο
     δικό μας σκοπό ήταν απογοητευτική.  Χρησιμοποιήσαμε ένα πολύ
     λεπτό wrapper, =sqlitedict={{{ref(sqlitedict)}}}, για να πάρουμε
     ένα key-value interface to SQLite – μια relational βάση
     δεδομένων. Ο σχετικός WikipediaBase κώδικας είναι πολύ σύντομος:

     #+BEGIN_SRC python
       from sqlitedict import SqliteDict

         class SqlitePersistentDict(EncodedDict):
             def __init__(self, filename, configuration=configuration):
                 if not filename.endswith('.sqlite'):
                     filename += '.sqlite'

                 db = SqliteDict(filename)
                 super(SqlitePersistentDict, self).__init__(db)

             def sync(self):
       self.db.close()
       super(SqlitePersistentDict, self).sync()
     #+END_SRC


     Παρακάτω είναι δυο benchmark functions που θα διαβάσουν και θα
     γράψουν 1000000 φορές στην βάση.


     #+BEGIN_SRC python
       def benchmark_write(dic, times=100000):
             for i in xrange(times):
                 dic['o' + str(i)] = str(i) * 1000

         def benchmark_read(dic, times=100000):
             for i in xrange(times):
       dic['o' + str(i)]
     #+END_SRC

     Και παρακάτω φαίνεται πως συγκρίνονται τα διάφορα backends
     χρησιμοποιώντας αυτές τις δυο συναρτήσεις.

     #+BEGIN_SRC python
       >>> import timeit
       >>> sqlkv = SqlitePersistentDict('/tmp/bench1.sqlite')
       >>> timeit.timeit(lambda : benchmark_write(sqlkv), number=100)
       10.847157955169678
       >>> timeit.timeit(lambda : benchmark_read(sqlkv), number=100)
       18.88098978996277
       >>> dbmkv = DbmPersistentDict('/tmp/bench.dbm')
       >>> timeit.timeit(lambda : benchmark_write(dbmkv), number=100)
       0.18030309677124023
       >>> timeit.timeit(lambda : benchmark_read(dbmkv), number=100)
       0.14914202690124512
     #+END_SRC

     Η DBM βαση δεδομένων είναι σχεδόν 10 φορές ταχύτερη από sqlite.
     Η διαφορά στην εκτέλεση οφείλεται στις διαφορετικές committing
     policies που έχουν μεταξύ τους.  Μπορεί να είναι δυνατόν να
     ρυθμιστεί το SQLite ώστε να είναι τόσο γρήγορο όσο η DBM αλλά όχι
     με κάποιον εύκολο τρόπο.

**** Άλλα backends

     Άλλα backends λαμβάνονται υπόψη, κυρίως το Redis το οποίο
     εφαρμόσθηκε αμέσως μετά την παράδοση της εργασίας από τον Alvaro
     Morales.  Ο λόγος που αρχικά δεν το χρησιμοποιήσαμε ήταν γιατί
     έχει μοντελοποιηθεί ως ένας server-client και προσθέτει
     περιπλοκότητα σε ένα τμήμα του συστήματος το οποίο πρέπει να
     είναι όσο το δυνατόν πιο απλό. Ένας άλλος λόγος του αρχικού
     προβληματισμού μας ήταν σχετικά με το ότι το redis είναι
     ανεξάρτητο project δηλαδή δεν είναι μέρος της python. Θεωρήσαμε
     πως ήταν καλύτερα να αποφευχθούν επιπλέον εξαρτήσεις ειδικά όταν
     είναι η cool database της ημέρας.

** Date parser

   Η κατανόηση ημερομηνιών υπάρχει σε ένα ξεχωριστό πακέτο που
   ονομάζεται =overlay-parse={{{ref(overlay_parse)}}}.

*** Parsing με overlays

    Η έννοια του =overlay= εμπνεύστηκε από τα =emacs
    overlays={{{ref(emacs_overlays)}}}. Είναι αντικείμενα που
    εξειδικεύουν την συμπεριφορά ενός υποσυνόλου του κειμένου με το να
    του δίνουν ιδιότητες για παράδειγμα το κάνουν clickable ή
    highlighted.

    Ένα overlay επί ενός μέρους ενός κείμενου \(t\) στο πλαίσιο μας είναι:

    - Ένα ζευγάρι που ορίζει την έκταση του υπο-κείμενου
    - ένα σύνολο από ετικέτες (tag set) που ορίζουν τα εννοιολογικά
      σύνολα στα οποία εμπίπτει το συγκεκριμένο υποκείμενο.
    - Αυθαίρετες πληροφορίες (τύπου \(Α\)) που το συγκεκριμένο
      υποκείμενο εκφράζει.

    Πιο αυστηρά:


    #+BEGIN_EXPORT latex
    \begin{align*}
    & o_i \in TextRange\(t\) \times Set(Tag) \times A \\
    & Text \rightarrow \left\{o_1, o_2, ..., o_n\right\}
    \end{align*}
    #+END_EXPORT

    Για παράδειγμα, από το παρακάτω κείμενο

    #+BEGIN_EXPORT latex
    \[
    The\,weather\,today,\,
    \overbrace{Tuesday}^\text{\(o_1\)} \,
    \overbrace{21^{st}}^\text{\(o_2\)} \, of \,
    \overbrace{November}^\text{\(o_3\)} \,
    \overbrace{2016}^\text{\(o_4\)}, \, was \, sunny.
    \]
    #+END_EXPORT

    Μπορούμε να εξάγουμε overlays \(\left\{o_1, ... , o_4\right\}\) έτσι ώστε

    #+BEGIN_EXPORT latex
    \[
    \begin{array}[b]{rlll}
    o_1 = (&r("Tuesday"),  & \{\mathrm{DayOfWeek}, \mathrm{FullName}\}, & 2) \\
    o_2 = (&r("21^{st}"),   & \{\mathrm{DayOfMonth}, \mathrm{Numeric}\}, & 21) \\
    o_3 = (&r("November"), & \{\mathrm{Month}, \mathrm{FullName} \}, & 11) \\
    o_4 = (&r("2016"),     & \{\mathrm{Year}, \mathrm{4digit} \}, & 2016)
    \end{array}
    \]
    #+END_EXPORT

    Παρατηρείστε ότι όλα τα overlays του παραδείγματος έχουν \(A =
    \mathbb{N}\), όπως κωδικοποιούμε την ημέρα της εβδομάδος, τη μέρα
    του μήνα, το μήνα του έτους ως φυσικούς αριθμούς.  Κωδικοποιούμε
    πιο ακριβή πληροφορία (πχ αυτή η μέρα είναι διαφορετική από την
    μήνα από την φύση της) στο σύνολο των ετικετών (tag sets).

    Μόλις έχουμε ένα σύνολο από overlays μπορούμε να ορίσουμε μια
    overlay sequences ως overlays τα οποία έχουν συνεχόμενο εύρος,
    Αυτά και τα δικά τους tag sets ταυτίζονται με ειδικά μοτίβα .  Για
    παράδιγμα μπορούμε να ψάξουμε για σειρές από overlays που
    ταιριάζουν με το pattern

    \[
    p = \mathrm{DayOfMonth}, \mathrm{Separator(/)}, (\mathrm{Month} \wedge \mathrm{Number}), \mathrm{Separator(/)}, \mathrm{Year}
    \]

    ταιριάζει patterns όπως \(22/07/1991\), οπού \(Separator(/)\)
    ταιριάζει μονό με τον χαρακτήρα "/"

*** Το παράδειγμα των ημερομηνιών

    Η βασική εφαρμογή που θα χρησιμοποιήσουμε ως παράδειγμα για τη
    λειτουργία των overlays είναι η κατανόηση ημερομηνιών. Το =dates=
    sumbmodule έχει 2 βασικά entry points:

    - =just_dates= που ψάχνει για ημερομηνίες σε ένα κείμενο.
    - =just_ranges= που ψάχνει για εύρη ημερομηνιών σε ένα κείμενο.

    Παρακάτω παρουσιάζονται κάποια παραδείγματα. Σημειώστε πως =0=
    σημαίνει =unspecified=

    #+BEGIN_SRC python
      >>> from overlay_parse.dates  import just_dates, just_ranges, just_props
      >>> just_dates("Timestamp: 22071991: She said she was \
              coming on april the 18th, it's 26 apr 2014 and hope is leaving me.")
      ... [(22, 7, 1991), (18, 4, 0), (26, 4, 2014)]
      >>> dates = just_dates("200 AD 300 b.c.")
      >>> just_dates("200 AD 300 b.c.")
      [(0, 0, 200), (0, 0, -300)]
      >>> just_ranges(u"I will be there from 2008 to 2009")
      [((0, 0, 2008), (0, 0, 2009))]
      >>> just_ranges("I will stay from July the 20th until today")
      [((20, 7, 0), (29, 4, 2016))]
      >>> just_dates('{{Birth date and age|1969|7|10|df=y}}')
      [(10, 7, 1969)]
      >>> just_ranges(u'German: [\u02c8v\u0254lf\u0261a\u014b ama\u02c8de\u02d0\u028as \u02c8mo\u02d0tsa\u0281t], English see fn.;[1] 27 January 1756\xa0\u2013 5 December 1791')
      [((27, 1, 1756), (5, 12, 1791))]
    #+END_SRC

** Παραρτήματα

*** Παράδειγμα python unit test


    #+BEGIN_SRC python
     class TestFetcher(unittest.TestCase):
         def setUp(self):
             self.fetcher = fetcher.get_fetcher()

         def test_html(self):
             html = self.fetcher.html_source("Led Zeppelin")
             self.assertIn("Jimmy Page", html)

         def test_markup_source(self):
             src = self.fetcher.markup_source("Led Zeppelin")
             self.assertIn("{{Infobox musical artist", src)


         def test_unicode_html(self):
             html = self.fetcher.html_source(u"Rhône")
             self.assertIn("France", html)

         def test_unicode_source(self):
             src = self.fetcher.markup_source("Rhône")
             self.assertIn("Geobox|River", src)

         def test_silent_redirect(self):
             # redirects are only supported when force_live is set to True
             src = self.fetcher.markup_source("Obama", force_live=True)
             self.assertFalse(re.match(fetcher.REDIRECT_REGEX, src))
    #+END_SRC

*** Παράδειγμα εκτέλεσης ενός python test

    ** Python test runs


    #+BEGIN_SRC sh
     $ python setup.py test -s tests.test_lispify
     running test
     running egg_info
     writing requirements to wikipediabase.egg-info/requires.txt
     writing wikipediabase.egg-info/PKG-INFO
     writing top-level names to wikipediabase.egg-info/top_level.txt
     writing dependency_links to wikipediabase.egg-info/dependency_links.txt
     writing entry points to wikipediabase.egg-info/entry_points.txt
     reading manifest file 'wikipediabase.egg-info/SOURCES.txt'
     reading manifest template 'MANIFEST.in'
     writing manifest file 'wikipediabase.egg-info/SOURCES.txt'
     running build_ext
     test_bool (tests.test_lispify.TestLispify) ... ok
     test_bool_with_typecode (tests.test_lispify.TestLispify) ... ok
     test_date_multiple_voting (tests.test_lispify.TestLispify) ... ok
     test_date_simple (tests.test_lispify.TestLispify) ... ok
     test_date_with_range (tests.test_lispify.TestLispify) ... ok
     test_dict (tests.test_lispify.TestLispify) ... ok
     test_dict_with_escaped_string (tests.test_lispify.TestLispify) ... ok
     test_dict_with_list (tests.test_lispify.TestLispify) ... ok
     test_double_nested_list (tests.test_lispify.TestLispify) ... ok
     test_error (tests.test_lispify.TestLispify) ... ok
     test_error_from_exception (tests.test_lispify.TestLispify) ... ok
     test_keyword (tests.test_lispify.TestLispify) ... ok
     test_keyword_with_typecode (tests.test_lispify.TestLispify) ... ok
     test_list (tests.test_lispify.TestLispify) ... ok
     test_list_of_dict (tests.test_lispify.TestLispify) ... ok
     test_list_of_dict_with_typecode (tests.test_lispify.TestLispify) ... ok
     test_list_with_typecode (tests.test_lispify.TestLispify) ... ok
     test_nested_list (tests.test_lispify.TestLispify) ... ok
     test_none (tests.test_lispify.TestLispify) ... ok
     test_none_with_typecode (tests.test_lispify.TestLispify) ... ok
     test_number (tests.test_lispify.TestLispify) ... ok
     test_number_with_typecode (tests.test_lispify.TestLispify) ... ok
     test_string (tests.test_lispify.TestLispify) ... ok
     test_string_escaped (tests.test_lispify.TestLispify) ... ok
     test_string_not_keyword (tests.test_lispify.TestLispify) ... ok
     test_string_with_typecode (tests.test_lispify.TestLispify) ... ok
     test_unicode_string (tests.test_lispify.TestLispify) ... ok

     ----------------------------------------------------------------------
     Ran 27 tests in 0.047s

     OK
    #+END_SRC

*** Βρίσκοντας συνώνυμα με MySQL



    #+BEGIN_SRC sql
      mysql> select page_title, rd_title from \
      redirect join page on
      rd_from = page_id and
      (rd_title = "Bill_Clinton" or page_title = "Bill_Clinton");
      (rd_title = "Bill_Clinton" or page_title = "Bill_Clinton");
      +-------------------------------------+--------------+
      | page_title                          | rd_title     |
      +-------------------------------------+--------------+
      | BillClinton                         | Bill_Clinton |
      | William_Jefferson_Clinton           | Bill_Clinton |
      [.. see below for a formated verison of the data ...]
      | William_Jefferson_Clinton           | Bill_Clinton |
      +-------------------------------------+--------------+
      46 rows in set (11.77 sec)
    #+END_SRC


    #+ATTR_LATEX: :environment longtable
    | =page_title=                          | =rd_title=     |
    |---------------------------------------+----------------|
    | =BillClinton=                         | =Bill_Clinton= |
    | =William_Jefferson_Clinton=           | =Bill_Clinton= |
    | =President_Clinton=                   | =Bill_Clinton= |
    | =William_Jefferson_Blythe_IV=         | =Bill_Clinton= |
    | =Bill_Blythe_IV=                      | =Bill_Clinton= |
    | =Clinton_Gore_Administration=         | =Bill_Clinton= |
    | =Buddy_(Clinton's_dog)=               | =Bill_Clinton= |
    | =Bill_clinton=                        | =Bill_Clinton= |
    | =William_Jefferson_Blythe_III=        | =Bill_Clinton= |
    | =President_Bill_Clinton=              | =Bill_Clinton= |
    | =Bull_Clinton=                        | =Bill_Clinton= |
    | =Clinton,_Bill=                       | =Bill_Clinton= |
    | =William_clinton=                     | =Bill_Clinton= |
    | =42nd_President_of_the_United_States= | =Bill_Clinton= |
    | =Bill_Jefferson_Clinton=              | =Bill_Clinton= |
    | =William_J._Clinton=                  | =Bill_Clinton= |
    | =Billl_Clinton=                       | =Bill_Clinton= |
    | =Bill_Clinton\=                       | =Bill_Clinton= |
    | =Bill_Clinton's_Post_Presidency=      | =Bill_Clinton= |
    | =Bill_Clinton's_Post-Presidency=      | =Bill_Clinton= |
    | =Klin-ton=                            | =Bill_Clinton= |
    | =Bill_J._Clinton=                     | =Bill_Clinton= |
    | =William_Jefferson_"Bill"_Clinton=    | =Bill_Clinton= |
    | =William_Blythe_III=                  | =Bill_Clinton= |
    | =William_J._Blythe=                   | =Bill_Clinton= |
    | =William_J._Blythe_III=               | =Bill_Clinton= |
    | =Bil_Clinton=                         | =Bill_Clinton= |
    | =WilliamJeffersonClinton=             | =Bill_Clinton= |
    | =William_J_Clinton=                   | =Bill_Clinton= |
    | =Bill_Clinton's_sex_scandals=         | =Bill_Clinton= |
    | =Billy_Clinton=                       | =Bill_Clinton= |
    | =Willam_Jefferson_Blythe_III=         | =Bill_Clinton= |
    | =William_"Bill"_Clinton=              | =Bill_Clinton= |
    | =Billll_Clinton=                      | =Bill_Clinton= |
    | =Bill_Klinton=                        | =Bill_Clinton= |
    | =William_Clinton=                     | =Bill_Clinton= |
    | =Willy_Clinton=                       | =Bill_Clinton= |
    | =William_Jefferson_(Bill)_Clinton=    | =Bill_Clinton= |
    | =Bubba_Clinton=                       | =Bill_Clinton= |
    | =MTV_president=                       | =Bill_Clinton= |
    | =MTV_President=                       | =Bill_Clinton= |
    | =The_MTV_President=                   | =Bill_Clinton= |
    | =Howard_G._Paster=                    | =Bill_Clinton= |
    | =Clintonesque=                        | =Bill_Clinton= |
    | =William_Clinton=                     | =Bill_Clinton= |
    | =William_Jefferson_Clinton=           | =Bill_Clinton= |



*** Παράδειγμα κατηγοριών άρθρων

    Το άρθρο που αναφέρεται στον Leonardo DiCaprio εντασσεται στις
    επόμενες κατηγορίες (με bold είναι η κατηγορία που χρησιμοποιεί το
    WikipediaBase για να αποφασίσει πως το άρθρο αναφέρεται σε
    άνθρωπο).

    - Leonardo DiCaprio
    - 1974 births
    - *Living people*
    - 20th-century American male actors
    - 21st-century American male actors
    - American environmentalists
    - American film producers
    - American male child actors
    - American male film actors
    - American male soap opera actors
    - American male television actors
    - American people of German descent
    - American people of Italian descent
    - American people of Russian descent
    - American philanthropists
    - Best Actor AACTA Award winners
    - Best Actor Academy Award winners
    - Best Drama Actor Golden Globe (film) winners
    - Best Musical or Comedy Actor Golden Globe (film) winners
    - California Democrats
    - Film producers from California
    - Formula E team owners
    - Male actors from Hollywood, California
    - Male actors from Palm Springs, California
    - Male actors of Italian descent
    - People from Echo Park, Los Angeles
    - Silver Bear for Best Actor winners

      Οι κατηγορίες αυτές μοιάζουν ως εξής στο άρθρο

      #+CAPTION: The rendered list of categores for Leonardo DiCaprio
      #+NAME:   fig:dicaprio-categories
      #+attr_latex: :placement [H] :width \textwidth
      [[./dicaprio_categories.png]]


* WikipediaMirror

  Wikipedia mirror είναι ένα σύστημα με στόχο να αυτοματοποιήσει τη
  δημιουργία ενός τοπικού κλώνου της wikipedia περιέχοντας μόνο τα
  άρθρα --- δεν περιέχει τους χρήστες , συζήτηση και επεξεργασμένη
  ιστορία. Η αυτοματοποιημένη διαδικασία περιλαμβάνει τη ρύθμιση ενός
  διακομιστή, μια βάση δεδομένων και γέμισμα αυτής της βάσης δεδομένων
  με τα άρθρα της wikipedia.  Ο σκοπός για αυτό είναι να παρέχει την
  δυνατότητα πρόσβασης του συνόλου των δεδομένων της Wikipedia,
  ανεξάρτητα από το wikipedia.org .

** Mediawiki stack overview

   To wikipedia-mirror βασίζεται στο MediaWiki stack που παρέχεται από
   το Bitnami, μια υπηρεσία που χτίζει το σύνολο του διακομιστή εντός
   των ορίων ενός direcotry. Αυτό είναι χρήσιμο γιατί αποφεύγεται η
   επιβάρυνση της χρήσης container ή VM τεχνολογίας και μας δίνει τη
   δυνατότητα να έχουμε άμεση πρόσβαση στο σύστημα αρχείων του stack,
   ενώ εξακολουθούσαμε να έχουν το σύστημα κατασκευής Bitnami να κάνει
   την κοπιώδη εργασία της ενορχήστρωσης των διαφόρων τμημάτων και
   επίσης διαχωρίζεται ο διακομιστής από το υπόλοιπο συστήματος.

   Το stack αποτελείται από

   - Έναν http server, στην περίπτωση μας τον apache {{{ref(apache)}}}
   - Ένα web application runtime, στην περίπτωση μας PHP{{{ref(php)}}}
   - Μια βάση δεδομένων, στην περίπτωση μας η MySQL
   - Το ίδιο το web application, δηλαδή mediawiki

   Όλα τα παραπάνω παρέχονται από το bitnami mediawiki stack. Το
   Xampp{{{ref(xampp)}}} παλιότερα ήταν αποδεκτά η καλύτερη επιλογή
   αλλά είναι unmaintained, έτσι αποφασίσαμε να χρησιμοποιήσουμε το
   bitnami το οποίο δουλεύει αρκετά καλά.

   Όταν το stack ρυθμιστεί κατάλληλα, το wikipedia dump xml κατεβαίνει
   και μετατρέπεται σε sql dump με =mwdumper={{{ref(mwdumper)}}}.  Θα
   μπορούσε να piped άμεσα στο MySQL αλλά η εξαγωγή παίρνει χρόνο τα
   πράγματα μπορεί να χειροτερέψουν κατα το dumping.


*** Στοιχεία του stack.

    Παρουσιάζεται κάθε στοιχείο του stack με περισσότερες λεπτομέρειες
    παρακάτω.

****  Apache
     Σύμφωνα με τη wikipedia:


     #+BEGIN_EXAMPLE
      The Apache HTTP Server, colloquially called Apache, is the world's
      most used web server software. Originally based on the NCSA HTTPd
      server, development of Apache began in early 1995 after work on the
      NCSA code stalled. Apache played a key role in the initial growth of
      the World Wide Web, quickly overtaking NCSA HTTPd as the dominant HTTP
      server, and has remained most popular since April 1996. In 2009, it
      became the first web server software to serve more than 100 million
      websites.

      Apache is developed and maintained by an open community of developers
      under the auspices of the Apache Software Foundation. Most commonly
      used on a Unix-like system (usually Linux), the software is available
      for a wide variety of operating systems besides Unix, including
      eComStation, Microsoft Windows, NetWare, OpenVMS, OS/2, and
      TPF. Released under the Apache License, Apache is free and open-source
      software.
     #+END_EXAMPLE



     Είναι δίκαιο να πούμε ότι apache είναι ένα από ταους πιο
     δημοφιλείς διακομιστές web στο διαδίκτυο.  Η ίδια η wikipedia.org
     φαίνεται να χρησιμοποιεί ένα πιο σύνθετο stack που περιλαμβάνει
     varnish, ένα HTTP επιταχυντή, και nginx{{{ref(nginx)}}}, μια
     εναλλακτική λύση, επίσης αρκετά δημοφιλή διακομιστή HTTP.
     Καταλήξαμε σε αυτό το συμπέρασμα από την επιθεώρηση των headers
     που επιστρέφονται από τη wikipedia.org . Στην περίπτωση
     http://www.wikipedia.org ανακατευθυνόμαστε προς το secure domain
     (προσοχή στη γραμμή =Server:=):

     #+BEGIN_SRC sh
       $ curl -s -D - http://www.wikipedia.org -o /dev/null
       HTTP/1.1 301 TLS Redirect
       Server: Varnish
       [...]
     #+END_SRC

     Και αν ζητήσουμε κατ ευθείαν για το https://www.wikipedia.org

     #+BEGIN_SRC sh
       $ curl -s -D - https://www.wikipedia.org -o /dev/null
       HTTP/1.1 200 OK
       Server: nginx/1.9.4
       [...]
     #+END_SRC

     Ωστόσο, είναι πέρα από το πεδίο της συγκεκριμένης εργασίας να
     αναπαράγουμε με ακρίβεια υποδομή της Wikipedia . Έχουμε
     επικεντρωθεί στην λειτουργικότητα. Σ αυτό λόγω της δημοτικότητας,
     και της εν δυνάμει ταχύτητας των ρυθμών της αυτόματης
     εγκατάστασης το Bitnami MediaWiki stack χρησιμοποιήθηκε ως
     διακομιστή μας

**** PHP

     Η MediaWiki , η οποία συζητείται αργότερα, είναι γραμμένη εξ
     ολοκλήρου σε PHP, μια δημοφιλής πλευρά του server , με δυναμική
     δακτυλογράφηση, προσανατολισμένη στα αντικείμενα , γλώσσα
     scripting. Το PHP είναι απαραίτητο και είναι εγκατεστημένο με το
     Bitnami mediawiki stack. Το PHP είναι δημοφιλής ανάμεσα στους
     προγραμματιστές του web και αυτό οφείλεται εν μέρει στην
     υποστήριξη που έχει από πολλούς σχετικές βιβλιοθήκες με βάσεις
     δεδομένων (συμπεριλαμβανομένων PostgreSQL, MySQL  Microsoft
     SQL Server και SQLite ) και είναι ουσιαστικά ένα template
     δημιουργίας προτύπων γλώσσας HTML.


**** MySQL

     Mediawiki μπορεί να χρησιμοποιήσει πληθώρα SQL database backends:

     - *MSSQL:* Μια SQL βάση από τη Microsoft
     - *MySQL:* Χρησιμοποιοντας τη standard PHP library για MySQL.
     - *MySQLi:* Μια επέκταση του MySQL backend
     - *Oracle:* Μια αποκλειστικής εκμεταλλεύσεως SQL database από την Oracle.
     - *SQLite:* Μια SQL database που συνήθως χρησιμοποιείται ως
       βιβλιοθήκη αντί για client-server scheme όπως γίνεται με τις
       άλλες επιλογές της λίστας

     Η Wikipedia παρέχει πολλαπλά dump files για τους SQL πίνακες
     δευτερογενούς σημασίας στο MySQL format (eg. page redirects,
     categories etc) και προτείνει =mwdumper= που μετατρέπει XML
     dumpls των άρθρων της wikipedia σε MySQL. Αυτό και το ότι
     παρέχεται με το αυτοματοποιημένο stack του bitnami, κάνει το
     MySQL την προφανή επιλογήγια το wikipedia-mirror stack.

**** Mediawiki

     To Mediawiki είναι η καρδιά της wikipedia. Το MediaWiki είναι ένα
     free και open-source{{{ref(foss)}}} wiki
     application. Δημιουργήθηκε από το Wikimedia
     Foundation{{{ref(wikimedia)}}} και τρέχει πολλά δημοφιλή site
     όπως Wikipedia, Wikitionary{{{ref(wikitionary)}}} και Wikimedia
     Commons{{{ref(wikimedia_commons)}}}.

     Το λογισμικό έχει περισσότερα από 800 ρυθμίσεις και περισσότερα
     από 2.000 επεκτάσεις διαθέσιμες για τη διευκόλυνση διάφορα
     χαρακτηριστικά για να να προστεθεί ή να αλλάξει. Στη Wikipedia
     και μόνο, πάνω από 1000 αυτοματοποιημένη και έχουν ήμι -
     αυτοματοποιημένα bots και άλλα εργαλεία έχουν αναπτυχθεί για να
     βοηθήσουν στο moderation. Τα περισσότερα από αυτά δεν έχουν
     σημασία για τους δικούς μας σκοπούς. Οι χρήσιμες για μας
     επεκτάσεις είναι οι =Scriunto= και =parserfunctions=, και οι
     μόνες χρήσιμες ρυθμίσεις σχετίζονται με το όνομα της τοποθεσίας,
     το όνομα της βάσης δεδομένων κλπ και ως επί το πλείστον τις
     διαχειρίζεται το Bitnami


** Setting up

   Στη συνέχεια είναι βήμα προς βήμα οδηγείες για να στήσει κάνεις το
   wikipedia mirror. Πρώτα κατεβάζουμε τον κωδικά χρησιμοποιοντας το
   git{{{ref(git)}}}:


   #+BEGIN_SRC sh
     $ git clone https://github.com/fakedrake/wikipedia-mirror
     $ cd wikipedia-mirror
   #+END_SRC

   Σ' αυτό το σημείο θεωρητικά κάποιος μπορεί να τρέξει =make
   sql-load-dumps= τα οποία θα φροντίσουν να στηθεί οτιδήποτε
   χρειάζεται να φορτωθείη βάση δεδομένων σε μορφή dumps σε μια
   λειτουργική SQL βάση δεδομένων. Φυσικά για να γίνει αυτό πρώτα θα
   εκτελεσθούν μερικά βήματα.

   - Download the wikipedia database dumps in XML format.
   - Transform them into a format that MySQL understands.
   - Set up the bitnami stack that includes a local install of MySQL
   - Load the MySQL dumps into MySQL

   Όλα αυτά τα βήματα κωδικοποιούνται ως τμήμα μιας εξαρτώμενης
   ιεραρχίας κωδικοποιούμενα σε makefile targets και στη θεωρία αυτό
   πραγματοποιείται αυτόματα και αποτελεσματικά οδηγείται σε
   αποτελεσματική wikipedia mirror. Όμως αυτή λειτουργία είναι μεγάλη
   και ευθραυστη και συνιστάται κάθε βήμα να γίνεται εξατομικευμένα
   και χειροκίνητα.

   Πρώτα, κατεβάζουμε και εγκαθιστάμε το bitnami. Η ακόλουθη εντολή θα
   κατεβάσει έναν executable από το bitnami website και θα κάνει μια
   τοπική εγκατάσταση του bitnami stack όπως συζητήθηκε παραπάνω:

   #+BEGIN_SRC sh
     $ make bmw-install
   #+END_SRC

   Το επόμενο βήμα είναι να βεβαιωθούμε ότι το maven, η java είναι ένα
   is a software project management and comprehension είναι
   εγκαταστημένα, απαιτείται να εγκατασταθεί και να στηθεί το mwdumper
   (βλέπε παρακάτω). Μπορεί να γίνει αυτό αν βεβαιωθούμε ότι τα
   παρακάτω έχουν επιτευχθεί:

   #+BEGIN_SRC text
     $ mvn --version
   #+END_SRC

   Σημείωση: if running on Ubuntu 14.04, ίσως χρειασθεί να
   εγκαταστήσουμε το Maven (για Java) χρησιμοποιώντας =sudo apt-get
   install maven=.

   Τώρα όλα είναι έτοιμα για το αυτόματο download Wikipedia’s XML
   dumps{{{ref(wikipedia_dumps)}}} και στη συνέχεια τα μετατρέπει σε
   SQL χρησιμοποιώντας =mwdumper=. Πρώτα το mwdumper θα πρέπει να κατέβει
   και να χτισθεί.  Μετά τα συμπιεσμένα XML dumps θα πρέπει να κατέβουν
   από την wikipedia. Θα γίνουν uncompressed και τελικά θα μετατραπούν
   σ MySQL dumps χρησιμοποιώντας mwdumper. Αυτή είναι πολύ χρονοβόρα
   διαδικασία και χρειάζεται 6-11 ώρες σε ένα τυπικό μηχάνημα:


   #+BEGIN_SRC sh
     $ make sql-dump-parts
   #+END_SRC

   Όταν αυτο γίνει επιτυχώς μπορούμε να φορτώσουμε τα SQL dumps στη
   βάση δεδομένων MySQL

   #+BEGIN_SRC sh
     $ make sql-load-parts
   #+END_SRC


   Και τελικά

   #+BEGIN_SRC sh
     $ make mw-extensions
   #+END_SRC

** Mediawiki extensions

   Γιατη MediaWiki για να ενεργήσει όπως η wikipedia απαιτούνται μια
   σειρά από extensions. Η διαδικασία εγκατάστασης των εν λόγω
   extensions δεν είναι αυτοματοποιημένη ή streamline. Για να γίνει
   αυτόματη διαχείριση αυτής της πολυπλοκότητας ένας μηχανισμός
   παρέχεται την εγκατάσταση των extensions. Για υποστηρίξουμε επιπλέον
   για την wikipediabase πρέπει να προσθέσουμε τον ακόλουθο κώδικα στο
   =Makefile.mwextnesions= (τροποποιημένο αναλόγως)

   #+BEGIN_SRC makefile
     MW_EXTENSIONS += newextension
     mw-newextension-url = url/to/new/extnesion/package.tar.gz
     mw-newextension-php = NewExtensionFile.php
     mw-newextension-config = '$$phpConfigVariable = "value";'
   #+END_SRC

   Η wikipedia-mirror θα φροντίσει ώστε το extension να είναι ήδη
   εγκαταστημένο και εάν δεν είναι θα τοποθετήσει τα σωστά αρχεία στο
   σωτσό μέρος και θα διορθώσει τους κατάλληλους configuration
   files. Τα entry points για την διαχείριση των extensions είναι (με
   την προϋπόθεσή ότι το όνομά του εγγραφομένων extensions είναι
   newextension):

   #+BEGIN_SRC sh
     make mw-print-registered-extensions # Output a list of the registed extensions
     make mw-newextension-enable         # Install and/or enable the extension
     make mw-newextension-reinstall      # Reinstall an extension
     make mw-newextension-disable        # Disable the extension
     make mw-newextension-clean          # Remove the extension
   #+END_SRC


   Όλα τα registered extensions θα εγκατασταθούν και θα ενεργοποιηθούν
   όταν το wikipedia-mirror έχει χτισθεί.

** Φορτώνοντας τα mediawiki dumps

   Η Wikipedia παρέχει μηνιαία dumps όλων των βάσεων δεδομένων της. Το
   μεγαλύτερο μέρος των dumps είναι σε μορφή XML και πρέπει να
   κωδικοποιούνται σε MySQL να φορτωθούν στη βάση δεδομένων
   wikipedia-mirror. Υπάρχουν περισσότεροι από ένας τρόποι να το
   κάνουμε αυτό.

   Το Mediawiki πακετάρεται με ένα βοηθητικό πρόγραμμα για την εισαγωγή
   του XML dump. Ωστόσο, η χρήση του για την εισαγωγή ενός πλήρους
   wikipedia-mirror αποθαρρύνεται λόγω των περιορισμών επιδόσεων. Αντ
   αυτού προτείνονται εργαλεία όπως =mwdumper= που μετατρέπουν τα XML
   dumps σε MySQL ερωτήματα που γεμίζουν τη βάση δεδομένων.

   Το =mwdumper= είναι γραμμένο σε Java και αποστέλλονται χωριστά από
   MediaWiki και μπορεί να μετατρέψει τα δεδομένα μεταξύ των
   ακόλουθων μορφών:

   - XML
   - MySQL dump
   - SQLite dump
   - CSV

   Για τους σκοπούς μας έχει ενδιαφέρον μονό ο μετασχηματισμός από XML
   σε MySQL.

** COMMENT To bug στη βιβλιοθήκη xerces

   Πιθανότατα η μεγαλύτερη δυσκολία κατά τη δημιουργία του
   wikipedia-mirror ήταν η αντιμετώπιση ενός bug στο =mwdumper= --- το
   εργαλείο για τη μετατροπή των XML dumps σε SQL dumps --- αυτό κάνει
   το εργαλείο να αποτυγχάνει σε τυχαία άρθρα. Εφ όσον δεν μπορέσαμε
   να βρούμε τον κριβή λόγο που συμβαίνει αυτό το bug, το
   παρακάμπτουμε διαγράφοντας τα άρθρα που προκαλούν το πρόβλημα, και
   εφ όσον είναι ένα μεγάλο εμπόδιο σε μια κατά τα άλλα θεωρητικά
   πεπατημένη διαδικασία περιγράφουμε τη διαδικασία μας λεπρομερώς.

   Ιδού λοιπόν τι ακριβώς συμβαίνει: ενώ τρέχει το =make
   sql-dump-parts= δημιουργείται το παρακάτω exception:

   #+BEGIN_SRC text
     ...

     376,000 pages (14,460.426/sec), 376,000 revs (14,460.426/sec)
     377,000 pages (14,458.848/sec), 377,000 revs (14,458.848/sec)
     Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 2048
             at org.apache.xerces.impl.io.UTF8Reader.read(Unknown Source)
             at org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)
             at org.apache.xerces.impl.XMLEntityScanner.scanContent(Unknown Source)
             at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanContent(Unknown Source)
             at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)
             at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)
             at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
             at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
             at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
             at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
             at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source)
             at javax.xml.parsers.SAXParser.parse(SAXParser.java:392)
             at javax.xml.parsers.SAXParser.parse(SAXParser.java:195)
             at org.mediawiki.importer.XmlDumpReader.readDump(XmlDumpReader.java:88)
             at org.mediawiki.dumper.Dumper.main(Dumper.java:142)
     make: *** [/scratch/cperivol/wikipedia-mirror/drafts/wikipedia-parts/enwiki-20131202-pages-articles20.xml-p011125004p013324998.sql] Error 1
   #+END_SRC

   Διερευνώντας το πρόβλημα τρέχοντας =make --just-print
   sql-dump-parts= ανακαλύπτουμε πως η εντολή που αποτυγχάνει είναι:


   #+BEGIN_SRC sh
     $ java -jar /scratch/cperivol/wikipedia-mirror/tools/mwdumper.jar   --format=sql:1.5 /scratch/cperivol/wikipedia-mirror/drafts/wikipedia-parts/enwiki-20131202-pages-articles20.xml-p011125004p013324998.fix.xml > /root/path/wikipedia-parts//enwiki-20131202-pages-articles20.xml-p011125004p013324998.sql
   #+END_SRC

   Ευτυχώς αυτό δεν τρέχει για μεγάλο διάστημα έτσι ασφαλώς
   πειραματιζόμαστε.  Εδώ είναι το =time= output:

   #+BEGIN_SRC sh
     26.65s user 1.73s system 78% cpu 35.949 total
   #+END_SRC


   Το λάθος φαίνεται να συμβαίνει κατά την διάρκεια του διαβάσματος του
   XML dump έτσι δεν είναι ειδικό για το SQL output. Αυτό θα μπορούσε
   να είναι χρήσιμο για να διαπιστώσουμε ποιο άρθρο προκαλεί το λάθος,
   αποσύροντας το και ελπίζοντας να λυθεί το πρόβλημα. Για να το
   εντοπίσουμε κατ αρχάς προσπαθήσαμε να export σε XML:


   #+BEGIN_SRC sh
     $ java -jar /scratch/cperivol/wikipedia-mirror/tools/mwdumper.jar   --format=xml /scratch/cperivol/wikipedia-mirror/drafts/wikipedia-parts/enwiki-20131202-pages-articles20.xml-p011125004p013324998.fix.xml > /tmp/just-a-copy.xml
   #+END_SRC

   Όπως ήταν αναμενόμενο, το ίδιο λάθος εμφανίστηκε. Στη συνέχεια
   κοιτάμε τα τελευταία δύο άρθρα που έγιναν export τυπώνοντας με
   αντίστροφη σειρά το xml αρχείο που δημιουργήθηκε, βρίσκοντας τις
   τελευταίες δύο εμφανίσεις του =<title>= με =grep= και αναστρέφοντας
   ξανά για να τα τυπώσουμε με την αρχική σειρά (σημείωση ότι =tac=
   είναι όπως cat, μόνο πoυ εμφανίζει lines με αντίστροφη σειρά):

   #+BEGIN_SRC sh
     $ tac /tmp/just-a-copy.xml | grep "<title>" -m 2 | tac
           <title>The roaring 20s</title>
           <title>Cranopsis bocourti</title> # <- This is the last one
   #+END_SRC

   Αυτή η λειτουργία τελειώνει γρήγορα παρά το ότι το
   =/tmp/just-a-copy.xml= είναι αρκετά μεγάλο γιατί η =tac= ψάχνει το
   τέλος του αρχείου και διαβάζει προς τα πίσω μέχρι το =grep= να βρει
   τα 2 περιστατικά που ψάχνει για και κλείνει. Στο =ext3= η κατά την
   αναζήτηση δεν διασχίζεται ολόκληρο το αρχείο. Πράγματι, από τον
   κώδικα =tac=:

   #+BEGIN_SRC c
     if (lseek (input_fd, file_pos, SEEK_SET) < 0)
         error (0, errno, _("%s: seek failed"), quotef (file));
     /* Shift the pending record data right to make room for the new.
        The source and destination regions probably overlap.  */
     memmove (G_buffer + read_size, G_buffer, saved_record_size);
     past_end = G_buffer + read_size + saved_record_size;
     /* For non-regexp searches, avoid unnecessary scanning. */
     if (sentinel_length)
         match_start = G_buffer + read_size;
     else
         match_start = past_end;

     if (safe_read (input_fd, G_buffer, read_size) != read_size)
     {
         error (0, errno, _("%s: read error"), quotef (file));
         return false;
     }
   #+END_SRC

   Ασ σώσουμε την διαδρομή από το αρχικό xml αρχείο σε μια μεταβλητή
   γιατί το χρησιμοποιούμε πολύ.  Έτσι από δω και πέρα το
   =$ORIGINAL_XML= θα έχει διαδρομή από το αρχικό xml.


   #+BEGIN_SRC sh
     $ export ORIGINAL_XML=/scratch/cperivol/wikipedia-mirror/drafts/wikipedia-parts/enwiki-20131202-pages-articles20.xml-p011125004p013324998.fix.xml
   #+END_SRC

   Πρώτα ας δούμε αν κάτι περίεργο συμβαίνει με το xml αρχείο

   #+BEGIN_SRC sh
     $ grep "<title>Cranopsis bocourti</title>" -A 200 -B 100 $ORIGINAL_XML | less
   #+END_SRC

   Τίποτα περίεργο δεν βρέθηκε, έτσι δεν μπορούμε πραγματικά να
   διορθώσουμε το πρόβλημα in-place. Θα προσπαθήσουμε να αποσύρουμε
   βίαια ολόκληρο το άρθρο και ελπίζουμε ότι θα δουλέψει (spoiler
   alert: it does).

   Θα προσπαθήσουμε να επιθεωρήσουμε τους parents του τίτλου από το
   σπασμένο άρθρο.  Ευτυχώς το xml που δημιουργήθηκε είναι indented
   έτσι μπορούμε να βρούμε τους parents που βασιζόμενοι σε
   αυτό. Αριθμήσαμε 6 spaces από indentation έτσι θα ψάξουμε προς τα
   πίσω από εκεί σε κάθε επίπεδο του indentation. Η πρώτη γραμμή που
   βρήκαμε σε κάθε περίπτωση θα αποτελεί έναν άμεσο ancestor του
   άρθρου.

   #+BEGIN_SRC sh
     $ for i in {0..6}; do \
         echo "Level $i:"; \
         tac /tmp/just-a-copy.xml | grep "^ \{$i\}<[^/]" -m 1 -n | tac; \
     done

     Level 0:
     17564960:<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.3/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd" version="0.3" xml:lang="en">
     Level 1:
     Level 2:
     38:  <page>
     Level 3:
     Level 4:
     35:    <revision>
     Level 5:
     Level 6:
     26:      <text xml:space="preserve">&lt;!-- This article was auto-generated by [[User:Polbot]]. --&gt;
   #+END_SRC

   Φαίνεται ότι το xml είναι =page= σε ένα grand domain που ονομάζεται
   =mediawiki=. Θα μπορούσαμε να δούμε αυτό επίσης και από την java
   source αλλά αν και πιο ακριβό είναι πιο γρήγορο από το να
   χρησιμοποιούμε το source του mwdumper.  Ο πιο εύκολος τρόπος να
   κόψουμε αυτό το άρθρο θα ήταν το =awk= αλλά θα πάρει πολύ χρόνο και
   εμείς θέλουμε να βελτιστοποιήσουμε και να αυτοματοποιήσουμε την όλη
   διαδικασία. Πρώτα ας προσπαθήσουμε απλώς να συγκρίνουμε τα άρθρα:

   #+BEGIN_SRC sh
     $ cmp /tmp/just-a-copy.xml $ORIGINAL_XML
     /tmp/just-a-copy.xml /scratch/cperivol/wikipedia-mirror/drafts/wikipedia-parts/enwiki-20131202-pages-articles20.xml-p011125004p013324998.fix.xml differ: byte 2, line 1
   #+END_SRC

   Αυτό ήταν γρήγορο... Ας δούμε τι πήγε στραβά:

   #+BEGIN_SRC sh
     $ head $ORIGINAL_XML
     <mediawiki xmlns="http://www.mediawiki.org/xml/export-0.8/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.8/ http://www.mediawiki.org/xml/export-0.8.xsd" version="0.8" xml:lang="en">
       <siteinfo>
         <sitename>Wikipedia</sitename>
         <base>http://en.wikipedia.org/wiki/Main_Page</base>
         <generator>MediaWiki 1.23wmf4</generator>
         <case>first-letter</case>
         <namespaces>
           <namespace key="-2" case="first-letter">Media</namespace>
           <namespace key="-1" case="first-letter">Special</namespace>
           <namespace key="0" case="first-letter" />

     $ head /tmp/just-a-copy.xml
     <?xml version="1.0" encoding="utf-8" ?>
     <mediawiki xmlns="http://www.mediawiki.org/xml/export-0.3/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd" version="0.3" xml:lang="en">
       <siteinfo>
         <sitename>Wikipedia</sitename>
         <base>http://en.wikipedia.org/wiki/Main_Page</base>
         <generator>MediaWiki 1.23wmf4</generator>
         <case>first-letter</case>
         <namespaces>
           <namespace key="-2">Media</namespace>
   #+END_SRC

   Τα γνωρίσματα των xml tags είναι αρκετά διαφορετικά. Το καλύτερό
   μας αποτέλεσμα είναι άν τα line numbers συμπίπτουν. Μετρήσαμε τον
   αριθμό των γραμμών στο =/tmp/just-a-copy.xml= και ελπίζουμε ότι ο
   αντίστοιχος αριθμός των γραμμών στο =$ORIGINAL_XML= θα είναι ίδιος.
   Εάν αυτό συμβεί μπορούμε να αγνοήσουμε τις contextual xml
   πληροφορίες και να σβήσουμε το προβληματικό άρθρο. Θα
   χρησιμοποιήσουμε =wc= το οποίο είναι αρκετά γρήγορο.

   #+BEGIN_SRC sh
     $ wc -l /tmp/just-a-copy.xml
     17564961 /tmp/just-a-copy.xml
   #+END_SRC

   Και η αντίστοιχη γραμμή στο =$ORIGINAL_XML= είναι:


   #+BEGIN_SRC sh
     $ sed "17564960q;d" $ORIGINAL_XML
     [[Willie Jones (American football)|Willie Jones]],
   #+END_SRC

   Ποδόσφαιρο (football)... καμιά σχέση με τα βατράχια
   (frogs). Φαίνεται ότι δεν μπορούμε να αποφύγουμε κάποιο επίπεδο του
   parsing.

*** Parsing

    Θα κάνουμε τις ακόλουθες θεωρήσεις για να αποφύγουμε το κατάλληλο
    parsingτου εγγράφου:

    - Το XML στο αρχικό αρχείο είναι valid.
    - Κάθε XML μέσα στο άρθρο είναι HTML escaped

    Κατ αρχάς δουλεύοντας με γραμμές είναι αργή διαδικασία γιατί το
    user space code χρειάζεται να ψάχνει newlines. Δουλεύοντας με
    bytes αναθέτουμε εργασία στο kernel, επιταχύνοντας την εργασία
    σημαντικά. Έτσι το =dd= είναι το σωστό εργαλείο για την συγκεκριμένη
    δουλειά.  Έτσι πρώτα θα βρούμε σε πιο byte είναι το άρθρο που μας
    ενδιαφέρει

    #+BEGIN_SRC sh
      $ grep -b "<title>Cranopsis bocourti</title>" -m 1 $ORIGINAL_XML
      1197420547:    <title>Cranopsis bocourti</title>
    #+END_SRC

    Αυτό ίσως παρει καποιο χρόνο αλλά δυστυχώς είναι η μόνη μας
    επιλογή.  Η στρατιγική μας είναι να φτιάξουμε 2 αρχεία: το
    =/tmp/original_tail.xml= το οποίο να περιέχει όλα τα δεδομένα που
    υπάρχουν μετά τη σελίδα που θέλουμε να βγάλουμε και το
    =/tmp/original_head.xml= το οποίο περιέχει όλα τα δεδομένα πριν τη
    σελίδα που θέλουμε να βγάλουμε. Τώρα θα χρησιμοποιήσουμε =sed= να
    ψάξει για =</page>= μετά το byte 1197420547 το οποίο θα είναι το
    σημείο x βάζουμε το μέρος του =$ORIGINAL_XML= μετά στο σημείο =x=:

    #+BEGIN_SRC sh
      $ dd if=$ORIGINAL_XML skip=1197420547 ibs=1 | sed '0,/<\/page>/d' > /tmp/original_tail.xml
    #+END_SRC


    Θαυμάσια, αυτό δούλεψε! το =dd= δεν αντιγράφει αντίστροφα έτσι θα
    χρειαστεί να κάνουμε κάτι πιο περίπλοκο για να κατασκευάσουμε
    =/tmp/original_head.xml=.  Ας υποθέσουμε ότι η θέση που βρήκαμε
    τον τίτλο της σελίδας που θέλουμε να αφαιρέσουμε είναι \(\alpha =
    1197420547\) και το σημείο που η σελίδα αρχίζει είναι στο σημείο
    \(\beta\). Είναι ασφαλές να υποθέσουμε ότι \( \beta > \alpha -
    1000 \) (μπορούσαμε να αλλάξουμε τη σταθερά 1000 εάν αυτή η
    υπόθεση ήταν λάθος, αλλά τελικά δεν ήταν). Με αυτό τον τρόπο
    χρειάζεται μόνο να ψάξουμε στο 1Kb για τη συμβολοσειρά =<page>=.
    Αυτό θα ήταν ισοδύναμο με το εξής: αντί να κάνουμε copy τα bytes
    στο εύρος \([0, \beta)\), να συνδέσουμε δυο διαστήματα \(
    [0,\alpha - 1000] \cup (\alpha - 1000, \beta) \) δημιουργώντας ένα
    subshell το οποίο θα έχει πρώτο output το πρώτο εύρος και στη
    συνέχεια output \( (\alpha - 1000, \alpha) \) σταματόντας όταν
    βρεί =<page>=. Παρακάτω είναι ένα one liner:

    #+BEGIN_SRC sh
      $ (dd count=$((1197420547-1000)) ibs=1 if=$ORIGINAL_XML; \
         dd if=$ORIGINAL_XML count=1000 skip=$((1197420547-1000)) ibs=1 \
             | tac | sed '/<page>/,$d' | tac) > /tmp/original_head.xml
    #+END_SRC

    # There is some semi-interesting stuff going on when one parses
    # just the article. Maybe include that later...
    # https://github.com/infolab-csail/wikipedia-mirror/issues/3#issuecomment-40738778


*** Η τελική λύση

    Όλα τα παραπάνω χρησιμοποιήθηκαν για να συντεθεί ένα script που
    ζει στο =data/xml-parse.sh= το οποίο χρησιμοποιήθηκε από το
    makefiles να απομακρύνει όλα τα προβληματικά άρθρα. Εάν το
    =mwdumper= αποτύχει, ταυτοποιούμε το άρθρο που προκάλεσε το
    πρόβλημα και το απομακρύνουμε χρησιμοποιώντας =xml-parse.sh=. Στη
    συνέχεια ξανατρέχουμε το mwdumper.  Το επαναλαμβάνουμε αυτό μέχρι
    το mwdumper να πετύχει. Συνολικά τα προβληματικά άρθρα είναι
    περίπου 10-15, και είναι διαφορετικά ανάλογα με το dump που
    χρησιμοποιείται.

*** Καλύπτοντας με κενά

    Από την παραπάνω έκθεση των τρόπων που αντιμετωπίσουμε το θέμα του
    άρθρου που σπάει παραλείψαμε κάτι προφανές. Μια θεματικά
    διαφορετική προσέγγιση: το να καλύψουμε το άρθρο που προκαλεί το
    πρόβλημα με κενά. Μόλις εντοπίσουμε το εύρος στο οποίο η σελίδα
    βρίσκεται μπορούμε να κάνουμε =mmap= επακριβώς στο τμήμα του
    =$ORIGINAL_XML= και στη συνέχεια να κάνουμε =memset= καλύποντας το
    με χαρακτήρες κενών.  Η το πρόγραμμα ζει στο
    =data/page_remover.c=, Παρακάτω παρουσιάζουμε την κλήση στο
    =mmap=:

    #+BEGIN_SRC c
      ctx->off = off-pa_off;
      ctx->fd = open(fname, O_RDWR, 0x0666);
      if (ctx->fd == -1) {
          perror("open");
          return NULL;
      }

      ctx->size = len;
      ctx->data = mmap(0, len+ctx->off, PROT_READ | PROT_WRITE,
      	         MAP_SHARED, ctx->fd, pa_off);
      if (ctx->data == MAP_FAILED) {
          perror ("mmap");
          return NULL;
      }
    #+END_SRC

    και το =memset=

    #+BEGIN_SRC c
      /* You MIGHT want to thread this but I dont think it will make
       * much more difference than memset. */
      memset(ctx->data + ctx->off, ' ', ctx->size);
    #+END_SRC

    Περιέργως αυτό δεν έλυσε το πρόβλημα του mwdumper το οποίο δείχνει
    ότι μάλλον προκειται για memory leak από τη μεριά του =xerces= αλλά
    αυτό ξεπερνά τους στόχους της παρούσας εργασίας.

*** Η εντολή sed

    Παραπάνω αναφέραμε την χρήση της εντολής sed και ίσως να είναι
    χρήσιμο να το αναπτύξουμε περεταίρω. =sed= είναι ένα unix εργαλείο
    πού βρίσκεται στο πακέτο =coreutils= το οποίο σύμφωνα με το man
    page είναι ένας stream editor που φιλτράρει και μεταμορφώνει
    κείμενο. Η βασική λειτουργία είναι ότι το "pattern space", ή το
    input stream το οποίο είναι ένα unix stream --- που έρχεται από το
    αρχείο, ένα pipe ή απλά stdin --- περνά μέσα από ένα
    προγραμματίσιμο pipeline. Εκτυπώνεται είτε αυτούσιο το modified
    pattern space είτε --- με τη χρήση του =-n= flag -- επιλεγμένα
    τμήματα αυτού.  Ας δούμε τη χρήση που κάναμε παραπάνω για το sed.

    Αρχικά χρησιμοποιήσαμε sed για να εκτυπώσουμε μια μεμονωμένη line
    σε ένα αρχείο:

    #+BEGIN_SRC sh
      $ sed "17564960q;d"
    #+END_SRC


    Αυτό το =sed= πρόγραμμα διαχωρίζεται από semicolon. Το =sed=
    απαριθμεί τις γραμμές του input stream και τρέχει καθένα από τις
    εντολές διαχωρισμένες με ";" σε σειρά μέχρι μια να επιτύχει. Οι
    εντολές εδώ είναι =17564960q= και =d=. Η =17564960q= θα σταματήσει
    το sed όταν φτάσει στη γραμμή 17564960. Η =d= θα απορρίπτει την
    παρούσα. Έτσι το sed απορρίπτει γραμμές μέχρι να συναντήσει τη
    γραμμή 17564960 την οποία εκτυπώνει και τελειώνει.

    Χρησιμοποιούμε μια εντολή sed ως μέρος μιας σειράς εντολών shell
    piped όλες μαζί με στόχο να εκτυπωθούν όλες ο γραμμές ενός stream
    μετά από ένα συγκεκριμένο μοτίβο (στην περίπτωσή μας =</page>=).

    #+BEGIN_SRC sh
      $ sed '0,/<\/page>/d'
    #+END_SRC


    αυτή τη φορά είχαμε μόνο μια εντολή sed, =d=. To =sed= απαριθμεί
    στις γραμμές στο stream, απορρίπτοντας γραμμές στο εύρος των
    γραμμών από 0 μέχρι τη γραμμή που ταυτίζεται με το =<\/page>=,
    ουσιαστικά τυπώνοντας μόνο γραμμές μετά το =</page>=.

    Η τελική μας χρήση του =sed= είναι η αντίστροφη της προηγούμενης,

    #+BEGIN_SRC sh
      $ sed '/<page>/,$d'
    #+END_SRC

    Εδώ το sed απαριθμεί ξανά σε όλες τις lines τουstream. Αυτή τη
    φορά απορρίπτοντας γραμμές ανάμεσα στην πρώτη και αυτήν που
    ταιριάζει το =<page>= μέχρι την τελική γραμμή, σημειωμένη με =$=.

** Εργαλεία

   Ένας αριθμός εργαλείων αναπτύχθηκε για να βοηθήσουν τη διαδικασία
   του χειρισμού και της παρακολούθησης της διαδικασία του φορτώματος
   των dumps στη βάση. Παρουσιάζονται με λεπτομέρεια παρακάτω. Εφ όσον
   ο πηγαίος κώδικάς τους είναι συνοπτικός παρατίθεται ολόκληρος στο
   παράρτημα

*** utf8thread.c

    Το =utf8thread.c= είναι ένα άλλο χαμηλού επιπέδου πρόγραμμα το
    οποίο γεμίζει με κενά όλα τα invalid utf-8 characters από το
    αρχείο. Χρησιμοποιούμε =pthreads= για να επιταχύνουμε τα πράγματα.

*** webmonitor.py

    Το =webmonitor.py= είναι ένα python script το οποίο sets up μια
    σελίδα web page που δείχνει live δεδομένα σε μορφή ιστογράμματος
    για την πρόοδο του πληθυσμού της βάσης δεδομένων. =webmonitor.py=
    σερβίρει στατικές html σελίδες μετά της στέλνει δεδομένα μέσω
    websocket. Webmonitor μπορεί να δείχνει οποιοδήποτε stream από τα
    ζευγάρι =<epoc date> <float value>= που λαμβάνει στο input. Σαν
    παράδειγμα:

    #+BEGIN_SRC sh
    $ pip install tornado
    #+END_SRC

    Πρώτα πρέπει να εγκαταστήσουμε τα dependencies του script. Το
    οποίο μπορεί να είναι tornado, anasynchronous web framework
    supporting websockets. Δίνουμε οδηγίες tornado, tornado θα
    υπηρετεί τις ακόλουθες σελίδες:

    #+BEGIN_SRC html
      <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
      <html>
        <head>
          <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
          <title>DrNinjaBatmans Websockets</title>

          <script type="text/javascript" src="http://code.jquery.com/jquery-1.10.1.js"></script>
          <script type="text/javascript" src="http://code.highcharts.com/highcharts.js"></script>

          <script>
           var chart; // global
           var url = location.hostname + ':' + (parseInt(location.port));
           var ws = new WebSocket('ws://' + url + '/websocket');
           ws.onmessage = function(msg) {
               add_point(msg.data);
           };

           // ws.onclose = function() { alert('Connection closed.'); };

           var add_point = function(point) {
               var series = chart.series[0],
    	       shift = series.data.length > %d;

               chart.series[0].addPoint(eval(point), true, shift);
           };

           $(document).ready(function() {
               chart = new Highcharts.Chart(JSON.parse('%s'));
           });
          </script>

        </head>
        <body>
            <div id="container" style="width: 800px; height: 400px; margin: 0 auto"></div>
        </body>
      </html>
    #+END_SRC

    Με την έννοια αυτή η σελίδα αναμένεται να διαβάζει ένα stream of
    values από ένα websocket στο ws://localhost:8888/hostname – αν και
    είναι αρκετά έξυπνο να αλλάξουμε το localhost:8888 εάν υπηρετεί
    αυτό μια άλλη τοποθεσία – και να κάνουμε plot αυτά σε πραγματικό
    χρόνο χρησιμοποιώντας highcharts.js.  Ο πιθανός αναγνώστης ίσως
    παρατηρήσει ότι τα παραπάνω δεν είναι ακριβώς HTML αλλά
    περισσότερο ένα python formatted string. Αυτό συμβαίνει για 2
    λόγους. Πρώτον γιατί το First script handles το configuration
    (βλέπε chart = new Highcharts.Chart(JSON.parse(’%s’Δεύτερον , το
    πλάτος του graph υπολογίζεται σε page loadtime και το plot
    χρειάζεται να μετατιπισθεί για να δείξει μόνο τα πιο τελευταία
    σημεία.

    #+BEGIN_SRC sh
      $ for i in {1..100}; do echo $i;  sleep  1; done | \
          awk -oL "{print \$1/100}" | \
          python webmonitor.py
    #+END_SRC

    Αυτό θα παράγει σε διάστημα 1 δευτερολέπτου, αριθμούς από το 1 ως
    το100. Μετά τα normalizes χρησιμοποιώντας awk και τα τροφοδοτεί σε
    webmonitor. Αφού αυτή η εντολή εκτελεσθεί μπορούμε να ανοίξουμε
    τον browser και να κάνουμε navigate στο localhost:8888.

    Χρησιμοποιούμε αυτό για να ρυθμίσουμε από απόσταση το ολικό
    μέγεθος των δεδδομένωνπου το mysql καταναλώνει.


*** xml-parse.sh

    Άπλα αφαιρόντας συγκεκριμένα άρθρα διορθώνουμε το πρόβλημα του
    =xerces=. Αν τα άρθρα είναι απομονωμένα το επίσης error
    εξαφανίζεται. Το =xml-parse.sh= διαγράφει τα ζητούμενα άρθρα από
    το αρχείο xml.

    #+BEGIN_SRC sh
      xml-parse.sh <original-xml-file> <title_of_article_to_remove> [inplace]
    #+END_SRC

    αν το τελευταίο όρισμα είναι το =inplace=, τότε το
    =page_remover.c= θα χρησιμοποιηθεί για να καλύψει το άρθρο με
    κενά. Αυτή η διαδικασία είναι πολύ πιο γρήγορη. Διαφορετικά η
    σελίδα άπλα διαγράφεται και το αποτέλεσμα εμφανίζεται στο
    =stdout=. Μόλις το =xml-parse.sh= τελειώσει επιτυχώς μπορεί κάνεις
    να τρέξει:

    #+BEGIN_SRC sh
      java -jar tools/mwdumper.jar RESULTING_XML --format=sql:1.5 > SQL_DUMP
    #+END_SRC

*** sql-clear.sh

    Το =sql-clear.sh= είναι ένα μικρό bash script που "κουτσουρεύει"
    όλους τους πίνακες από την βάση δεδομένων.  Με τον όρο
    "κουτσουρευει" εννοούμε ότι αφήνει τα table scheamata ανεπηρέαστα
    και διαγράφει όλα τα internal δεδομένα.


*** page\_remover.c

    Όπως προηγουμένως συζητήθηκε η =xerces= βιβλιοθήκη την οποία
    χρησιμοποιεί το =mwdumper= απέτυχε, φαινομενικά τυχαία να
    επεξεργαστεί κάποιες σελίδες. Για να διευθετηθεί αυτό το πρόβλημα
    αφαιρέσαμε τις σελίδες πλήρως και ξαναπροσπαθήσαμε. Επειδή αυτή η
    εργασία είναι εύκολη άλλα αργή γράψαμε ένα χαμηλού επιπέδου
    πρόγραμμα στη C για να το επιλύσουμε, το =page_remove.c=. Το
    =page-remover= δέχεται ως input the path του XML wikipedia dump,
    το offset του άρθρου που θέλουμε να καλύψουμε και το μέγεθος του
    άρθρου.  Μετά χρησιμοποιεί το =mmap= system call για να αποκτήσει
    ψευδο-random-access στα δεδομένα μέσα στο αρχείο και γεμίζει το
    άρθρο με withespace characters. Το =page_remover.c= δεν είναι
    threaded μιας που το bottleneck είναι στο HDD IO speed και ο
    παραλληλισμός δεν θα βοηθούσε.

** Αυτοματισμός

   Δημιουργώντας μια wikipedia mirror ίσως φαίνεται μια απλή
   διαδικασία αλλά συμπεριλαμβάνει πολλές αγκαθωτές λεπτομέρειες και
   επαναλαμβανόμενα tasks. Πολλαπλές μέθοδοι αυτοματισμού εφαρμόσθηκαν
   για να ολοκληρώσουν μια μεγάλη ποικιλία tasks που
   συμπεριλαμβάνονται στην εκτέλεση.

*** Makefiles

    Το πιο σημαντικό μέρος του αυτοματισμού της wikipedia-mirror είναι
    το =make= build system. Make είναι ένα build system όπου κάποιος
    μπορεί να δηλώσει τα απαιτούμενα αρχεία (targets), dependencies
    για αυτά, και ένα σύνολο από shell commands που θα χτίσουν αυτά
    τα targets. Κάθε target είναι ουσιαστικά μια finite state machine
    με δύο καταστάσεις:

    - Ένα αρχείο που υπάρχει και είναι επικυροποιημένο με τα  dependencies και
    - Ένα αρχείο που είτε δεν υπάρχει ή η modification date είναι
      παλαιότερη από αυτό ή τουλάχιστον ενός από τα dependencies.

    Και μια σειρά από shell εντολές για την μεταφορά από την πρώτη στη
    δεύτερη κατάσταση.

    Για παράδειγμα , σώζουμε το ακόλουθο ως Makefile σε ένα project
    που περιέχει τα αρχεία =foo.c=, =foo.h=, =bar.c= και =bar.h=:


    αυτό σημαίνει ότι για να χτίσουμε το εκτελέσιμο =foobar=
    χρειαζόμαστε =foo.o= και =bar.o=. Και για να χτίσουμε =foo.o= και
    =bar.o= χρειαζόμαστε =foo.c= και =foo.h=, και =bar.c= και =bar.h=
    αντίστοιχα.

    Επίσης παρέχουμε εντολές για να χτισθεί το foo.o, bar.o και
    foobar, οι οποίες είναι

    - =gcc foo.c -c -o foo.o=
    - =gcc bar.c -c -o bar.o=
    - και =gcc foo.o bar.o -o foobar=

    αντίστοιχα. παρατηρούμε ότι δεν υπάρχουν κανόνες για τα =.c= και
    =.h= αρχεία. Αυτό συμβαίνει γιατί το =make= πρέπει να αποτυγχάνει
    εάν δεν είναι παρόντα. Έτσι εάν τρέχουμε το =make foobar=, το
    =make= θα ελέγχει για την ύπαρξη του =foobar= και την ημερομηνία
    της τροποποίησης. Εάν το =foobar= λείπει ή η ημερομηνία
    τροποποίησης είναι προηγούμενη από τις εξαρτήσεις του (δηλαδή
    =foo.o= και =bar.o=) αυτό θα ξαναχτιστεί. Εάν κάποια από
    εξαρτήσεις απουσιάζει η ίδια λογική ισχύει και για αυτή. Με αυτό
    τον τρόπο εάν χτίσουμε μια φορά το =foobar=, και μετά
    τροποποιήσουμε το =bar.c= και ξανατρέξουμε =make foobar=, το =make= θα
    θεωρήσει αναδρομικά ότι:

    - το =bar.o= είναι out of date όσον αφορά την εξάρτηση =bar.c=
    - Όταν =bar.o= έχει πλέον μια πιο πρόσφατη ημερομηνία μετατροπής
      από το =foobar= και για αυτό το τελευταίο είναι out of date όσον
      αφορά την dependencyτου =bar.o=, έτσι χρειάζεται να ξανχτιστεί.

    με αυτόν τον τρόπο το =make= πετυχαίνει μια σχεδόν βέλτιστη
    στρατηγική για την επίτευξη κάθε φορά του ελάχιστου ποσοστού των
    απαιτούμενων στόχων. Τώρα που ξεκαθαρίσαμε την βασική λογική των
    =make= ας κάνουμε πιο σαφή μερικά από τα βασικά χαρακτηριστικά
    τους που κάνουν τη ζωή μας πιο εύκολη.

**** Phony targets

     Μερικές εργασίες δεν είναι αρχεία και χρειάζονται να τρέχουν κάθε
     φορά που το make τις συμπεριλαμβάνει στο dependency tree. Γι αυτά
     έχουμε ένα ειδικό keyword .PHONY:.

     Παρακάτω είναι ένα παράδειγμα.

     # code
     #+BEGIN_SRC makefile
       .PHONY:
       clean:
           rm -rf *
     #+END_SRC

     Αυτό λέει στο =make= ότι κανένα αρχείο ονομαζόμενο clean δε θα
     δημιουργηθεί τρέχοντας =rm -rf *=, και επίσης ακόμα και εάν
     υπάρχει ένα up-to-date ονομαζόμενο αρχείο ονομαζόμενο =clean=,
     αυτό το target θα τρέχει ανεξάρτητα.

     Αξίζει να σημειώσουμε ότι οι phony εξαρτήσεις πάντα θα θεωρούνται
     out of date.

     Για παράδειγμα:

     #+BEGIN_SRC makefile
       .PHONY:
       say-hello:
           echo "hello"

       test.txt: say-hello
           touch test.txt
     #+END_SRC

     Όταν το =touch test.txt= θα τρέχει κάθε φορά που τρέχουμε =make
     test.txt= απλώς γιατί το =make= δεν μπορεί να γνωρίζει με
     βεβαιότητα ότι το phony target =say-hello= δεν άλλαξε τίποτε
     σημαντικό για το =test.txt=. Για αυτό το λόγο τα phony targets
     χρησιμοποιούνται για user facing tasks.

**** Variables

     Τα makefiles μπορούν να έχουν μεταβλητές ορισμένες με ποικίλους
     τρόπους. Μερικές περιπτώσεις που έχουν γίνει για να
     χρησιμοποιηθούν στην wikiepedia-mirror παρουσιάζονται παρακάτω.

***** COMMENT Αναδρομικές μεταβλητές

      #+BEGIN_SRC makefile
        OBJCETS = foo.o bar.o

        show:
            echo $(OBJECTS)
      #+END_SRC

      Τρέχοντας =make show= θα εμφανίσει =foo.o bar.o= στην
      κονσόλα. Όλες οι μεταβλητές αντικαθιστώνται με τις τιμές τους αν
      βάλει κάνεις πρενθέσεις γύρω από το όνομά τους και προθέσει ένα
      δολάριο (=$=). Οι μεταβλητέσ των makefiles δεν έχουν τύπους,
      αναφορά σε μια μεταβλητή είναι ισοδύναμη με string substitution,
      όπως είναι και στο shell scripting.

      Οι μεταβλητές που ορίζονται με ένα απλό =\== είναι recursively
      expanded. Αυτό σημαίνει ότι αφού το όνομα της μεταβλητής
      αντικαθίσταται από την τιμή της μια αναδρομική διαδικασία
      συνεχίζει να κάνει expand τις τιμές που προκύπτουν με την ίδια
      τη μεταβλητή ακόμα στο local scope.

      #+BEGIN_SRC makefile
        library = foo

        foo-libs = -lfoo
        foo-includes = -I./include/foo

        bar-libs = -lbar
        bar-includes = -I./include/bar

        libs = $($(library)-libs)
        includes = $($(library)-includes)

        waz:
           gcc waz.c $(includes) $(libs)
      #+END_SRC

      τρέχοντας =make=

      #+BEGIN_SRC makefile
        gcc waz.c $(includes) $(libs)
        gcc waz.c $($(library)-includes) $($(library)-libs)
        gcc waz.c $(foo-includes) $(foo-libs)
        gcc waz.c -I./include/foo -lfoo
      #+END_SRC


      Παρατηρήστε πως οι αναφορές στις μεταβλητές καθαυτές
      δημιουργήθηκαν.

      Μεταβλητές μπορούν επίσης να ορισθούν στην εντολή =make=

      #+BEGIN_SRC sh
        $ make --just-print library=bar
        gcc waz.c -I./include/bar -lbar
      #+END_SRC


***** Simple variables

      Μερικές φορές δεν είναι επιθυμητό για τις μεταβλητές να είναι
      expanded επ αόριστον:

      #+BEGIN_SRC makefile
        kurma = the world $(support1)
        animal1 = four elephants
        animal2 = tortoise
        support1 = supported by $(animal1) $(support2)
        support2 := supported by a $(animal2) $(support2)

        all:
           echo $(kurma)
      #+END_SRC

      Εδώ πορσπαθουμε να δημιουργήσουμε ένα άπειρο μήνυμα.

      #+BEGIN_SRC sh
        $ make --just-print
        Makefile:5: *** Recursive variable `support2' references itself (eventually).  Stop.
      #+END_SRC

      το σύστημα μεταβλητών δηλαδή είναι κατά κάποιον τρόπο
      total{{{ref(total_function)}}}, με άλλα λογία η εύρεση της τιμής
      μεταβλητών μπορεί να είναι αναδρομική άλλα πρέπει να
      τερματίζει. Μπορούμε να αποφύγουμε αυτόν τον περιορισμό
      ορίζοντας μεταβλητές με ~:=~:


      #+BEGIN_SRC sh
        make --just-print
        echo the world supported by four elephants supported by a tortoise
      #+END_SRC

***** Automatic variables

      Το Makefile επίσης ορίζει μερικές contextual μεταβλητές οι
      οποίες είναι ορισμένες. Οι πιο σημαντικές automatic variables
      που ορίζει το gnu make είναι οι ακόλουθες

      - =$@=: Το όνομα του αρχείου του target. Εάν το target είναι ένα
        archive member, τότε =$@= είναι το όνομα του archive
        αρχείου. Στο pattern rule που έχει πολλαπλά targets, $@ είναι
        το όνομα του οποιουδήποτε target που κάνει το rule’s recipe να
        τρέχει.
      - =$%=: Το όνομα τουtarget member, όταν το target είναι ένα
        archive member. Για παράδειγμα, εάν το target είναι
        =foo.a(bar.o)= τότε =$%= είναι =bar.o= και =$@= είναι
        =foo.a=. =$%= είναι άδειο όταν το target δεν είναι ένα archive
        member.
      - =$<=: Το όνομα του πρώτου prerequisite. Εάν το target πήρε το
        recipe του από έναν implicit rule, αυτό θα είναι το πρώτο
        prerequisite που προστέθηκε από το implicit rule.
      - =$?=: Τα ονόματα από όλες τις εξαρτήσεις που είναι νεότερα από
        το target, με κενά μεταξύ τους . Για τα prerequisites που
        είναι archive members, μόνο named member
        χρησιμοποιούνται(βλέπε Archives).
      - =$^=: Τα ονόματα όλων των prerequisites, με κενά μεταξύ
        τους. Για τα prerequisites τα οποία είναι archive members,
        μόνο των named member χρησιμοποιείται. ένα
        target έχει μόνο ένα prerequisite σε κάθε άλλο αρχείο από το
        οποίο εξαρτάται, αναξαρτήτως από το πόσες φορές κάθε αρχείο
        είναι καταχωρημένο ως ένα no matter how many times each file
        prerequisite. Έτσι εάν τοποθετήσουμε στη λίστα ένα
        prerequisite για περισσότερο από μια φορά για ένα target, η
        value του =$ˆ= περιέχει μόνο ένα αντίγραφο του ονόματος.

***** Συναρτήσεις

      Οι συναρτήσεις είναι παρόμοιες με μεταβλητές ως προς το ότι και
      αυτές γίνονται expand σε συμβολοσειρές. Η μόνη διαφορά είναι ότι
      επιδέχονται παραμέτρους.

      #+BEGIN_SRC makefile
        greet = "Hello $1 (from $2)"
        john-greets = $(call greet,$1,John)

        .PHONY:
        all:
              @echo $(call john-greets,Chris)
      #+END_SRC

      Εδώ η έξοδος είναι

      #+BEGIN_SRC sh
        $ make
        Hello Chris (from John)
      #+END_SRC

** Επιδόσεις


*** Compile time

*** Runtme

    Το Compile time  περιλαμβάνει το χρόνο που χρειάζεται για:

    - Κατέβασμα όλων των στοιχείων του wikipedia server
    - The bitnami stack
      - mwdumper
      - mediawiki-extensions
      - Εγκατάσταση και χτίσιμο αυτών των στοιχείων (~1 min)
      - Κατέβασμα των wikipedia dumps
      - Προεπεξεργασία των dumps (~10 mins)
      - Populating τη mysql βάση δεδομένων(~10 days)

    Τα Builds έγιναν στο d Infolab’s Ashmore. Τα system’s specs είναι
    σχετικά ψηλά σε γενικές γραμμές αλλά το bottleneck ήταν το disk IO
    έτσι λιγότερο από 1% από τις υπόλοιπες διαθέσιμες πηγές
    χρησιμοποιήθηκαν κατά την διάρκεια του MySQL database
    population. Συγκεκριμένα τα χαρακτηριστικά του ashmore είναι:

    - *CPU:* Xeon E5-1607 3GHz 4-Core 64 bit
    - *Main memory:* 64G
    - *HDD:* (spinning disk) 500GB + 2Tb

    Εφ όσον το βασικό bottleneck είναι η δημιουργία βάσης δεδομένων
    --- δηλαδή οι επιδόσεις της MySQL --- δόθηκε μεγάλη προσοχή και
    πειραματισμός στη σωστή ρύθμιση της βάσης, άλλα η επιτάχυνση ήταν
    εν τέλη ελάχιστη και έτσι τα περισσότερα απ' όσα δοκιμάστηκαν δεν
    περιλήφθηκαν στα Makefiles.

    Η backend engine που χρησιμοποιήσαμε για τη MySQL είναι η
    InnoDB. Μερικές από της μεθόδους βελτιστοποίησης που επιχειρήθηκαν
    παρουσιάζονται παρακάτω.

    - Ρύθμιση του
      =innodb_buffer_pool_size={{{ref(innodb_buffer_pool_size)}}}. Ενώ
      η διαθέσιμη μνήμη του ashmore είναι αρκετά μεγάλη, αυξάνοντας το
      buffer pool μέχρι και κάποια GB δεν είχε σοβαρό αντίκτυπο στην
      επίδοση.
    - Αλλάζοντας το =innodb_flush_method={{{innodb_flush_method}}} σε
      =O_DSYNC= για να αποφυγυμε κλήσεις στην =fsync=. Εν ολίγοις το
      πρόβλημα με την =fsync= είναι ότι ψάχνει σειριακά τις mapped
      σελίδες ενός αρχείου για dirty pages με αποτέλεσμα να γίνεται
      αργό για μεγάλα αρχεία.
    - Ρυθμίζοντας το
      =innodb_io_capacity={{{ref(innodb_io_capacity)}}}. Εν τέλη η
      τιμή της μεταβλητής ήταν υψηλότερη από το bandwidth του σκληρού
      δίσκου

    Η μόνη βελτιστοποίηση που είχε αισθητό αποτέλεσμα ήταν η αλλαγή
    του MySQL dump ώστε να θέτει

    #+BEGIN_SRC sql
      SET AUTOCOMMIT = 0; SET FOREIGN_KEY_CHECKS=0;
    #+END_SRC

    Αυτό επέτρεψε στο InnoDB να κάνει περισσότερη δουλειά στην κύρια
    μνήμη πριν επικοινωνήσει με το δίσκο και επίσης μείωσε τη συνολική
    δουλειά εμπιστευόμενος ότι οι τιμές των κελιών που αναφέρονταν σε
    άλλους πίνακες όντως έδειχναν κάπου.

** Επιδόσεις

* References
